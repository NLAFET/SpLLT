\documentclass{article}
\pagenumbering{arabic}

\usepackage{url}
\usepackage{color}
\usepackage{verbatim}
\usepackage{tikz}
\usepackage{gnuplot-lua-tikz}

\usepackage{xspace}

%\usepackage[utf8]{inputenc}
%\usepackage{fontspec}
\usepackage{pgfplots}


\usepackage[procnames]{listings}
\lstset{ %
  backgroundcolor=\color{gray98},    % choose the background color; you must add \usepackage{color} or \usepackage{xcolor}
  basicstyle=\tt\small, % \prettysmall      % the size of the fonts that are used for the code
  breakatwhitespace=false,          % sets if automatic breaks should only happen at whitespace
  breaklines=true,                  % sets automatic line breaking
  showlines=true,                  % sets automatic line breaking
  captionpos=b,                     % sets the caption-position to bottom
  commentstyle=\color{gray30},      % comment style
  extendedchars=true,               % lets you use non-ASCII characters; for 8-bits encodings only, does not work with UTF-8
  frame=single,                     % adds a frame around the code
  keepspaces=true,                  % keeps spaces in text, useful for keeping indentation of code (possibly needs columns=flexible)
  keywordstyle=\color{amblu},       % keyword style
  procnamestyle=\color{amred},       % procedures style
  language=[95]fortran,             % the language of the code
  numbers=left,                     % where to put the line-numbers; possible values are (none, left, right)
  numbersep=5pt,                    % how far the line-numbers are from the code
  numberstyle=\tiny\color{gray20}, % the style that is used for the line-numbers
  rulecolor=\color{gray20},          % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. comments (green here))
  showspaces=false,                 % show spaces everywhere adding particular underscores; it overrides 'showstringspaces'
  showstringspaces=false,           % underline spaces within strings only
  showtabs=false,                   % show tabs within strings adding particular underscores
  stepnumber=2,                     % the step between two line-numbers. If it's 1, each line will be numbered
  stringstyle=\color{amblu},       % string literal style
  tabsize=2,                        % sets default tabsize to 2 spaces
  % title=\lstname,                    % show the filename of files included with \lstinputlisting; also try caption instead of title
  procnamekeys={call}
}

\usepackage{color}
\definecolor{gray98}{rgb}{0.98,0.98,0.98}
\definecolor{gray20}{rgb}{0.20,0.20,0.20}
\definecolor{gray25}{rgb}{0.25,0.25,0.25}
\definecolor{gray16}{rgb}{0.161,0.161,0.161}
\definecolor{gray60}{rgb}{0.6,0.6,0.6}
\definecolor{gray30}{rgb}{0.3,0.3,0.3}
\definecolor{bgray}{RGB}{248, 248, 248}
\definecolor{amgreen}{RGB}{77, 175, 74}
% \definecolor{amblu}{RGB}{72, 88, 102}
\definecolor{amblu}{RGB}{55, 126, 184}
\definecolor{amred}{RGB}{228,26,28}
\definecolor{amyellow}{RGB}{237,177,32}
\definecolor{ampurple}{RGB}{126,47,142}
\newcommand{\mye}[1]{\textcolor{amyellow}{#1}\xspace}
\newcommand{\mgr}[1]{\textcolor{amgreen}{#1}\xspace}
\newcommand{\mbl}[1]{\textcolor{amblu}{#1}\xspace}
\newcommand{\mre}[1]{\textcolor{amred}{#1}\xspace}
\newcommand{\mbk}[1]{\textcolor{black}{#1}\xspace}
\newcommand{\mbp}[1]{\textcolor{ampurple}{#1}\xspace}

\input{nlafet_style.sty}

\newcommand{\alert}[1]{\textcolor{red}{#1}\xspace}
\newcommand{\starpu}{{StarPU}\xspace}
\newcommand{\parsec}{{PaRSEC}\xspace}
\newcommand{\TODO}[1]{\alert{TODO: #1}\xspace}
\newcommand{\openmp}{OpenMP\xspace}
\newcommand{\ma}{HSL\_MA87\xspace}
\newcommand{\spllt}{SpLLT\xspace}
\newcommand{\qrm}{\texttt{qr\_mumps}\xspace}
\newcommand{\pastix}{\texttt{PaSTIX}\xspace}
\newcommand{\nb}{\texttt{nb}\xspace}

\renewcommand{\theequation}{\arabic{section}.\arabic{equation}}
\renewcommand{\thetable}{\arabic{section}.\arabic{table}}
\renewcommand{\thefigure}{\arabic{section}.\arabic{figure}}
\newcommand{\lsection}[1]{\section{#1} \setcounter{equation}{0} \setcounter{figure}{0} \setcounter{table}{0} \label{#1}}
\newcommand{\lsubsection}[1]{\subsection{#1} \label{#1}}
\newcommand{\lsubsubsection}[1]{\subsubsection{#1} \label{#1}}

\bibliographystyle{siam}

%-----------------------------------------------------------------------
%
% include macros
%
\input nlafet_macros.tex
%-----------------------------------------------------------------------



\newcommand{\stfccovertitle}
{Experiments with sparse Cholesky using runtime systems}

\newcommand{\theabstract}{We describe the development of a prototype
  code for the solution of large sparse symmetric positive definite
  systems that is efficient on parallel architectures. We implement a
  DAG-based Cholesky factorization that offers good performance and
  scalability on multicore architectures.  Our approach uses a runtime
  system to execute the DAG corresponding to the factorization
  algorithm. The runtime system plays the role of a software layer
  between the application and the architecture and handles the
  management of task dependencies as well as the task scheduling. In
  this model, the application is expressed with a high-level API,
  independent of the hardware details, thus enabling portability
  across different architectures. Although already widely used in the
  context of dense linear algebra, this approach is nevertheless
  challenging in the context of sparse algorithms because of the
  irregularity and variable granularity of the DAGs arising in these
  systems.  We assess the ability of two different Sequential Task
  Flow implementations to address this challenge: one implemented with
  the \openmp standard, and the other with the modern runtime system
  \starpu. We compare these implementations to the state-of-the-art
  \ma solver and demonstrate comparable performance on a multicore
  architecture. }

\textwidth  16.18cm
\textheight 23.4cm
\oddsidemargin -0.2mm
\evensidemargin -0.2mm
\def\baselinestretch{1.1}
\topmargin -8.4mm

\newcommand{\n}[1][1]{\vspace*{#1\baselineskip}\noindent}
\newcommand{\ISD}[1]{\begin{center}\n[.4]\textcolor{red}{ISD\
 \hspace*{0.15em} \fbox{\parbox{.8\textwidth}{#1}}}\n[.4]\end{center}}
\newcommand{\HSD}[1]{\begin{center}\n[.4]\textcolor{green}{FL\
\hspace*{0.15em} \fbox{\parbox{.9\textwidth}{\texttt{#1}}}}\n[.4]\end{center}}
\newcommand{\ASH}[1]{\begin{center}\n[.4]\textcolor{brown}{JH\
\hspace*{0.15em} \fbox{\parbox{.9\textwidth}{\texttt{#1}}}}\n[.4]\end{center}}

\newcommand{\metis}{{\sc Me$\!$T$\!$iS\ }}

\hyphenpenalty=10000
\widowpenalty=10000
\sloppy

\begin{document}


\begin{titlepage}

\vspace*{-0.5cm}

\vspace{1.0 cm}

{\Large \bf
\begin{center}
   \stfccovertitle
\end{center}}

\begin{center}
\mbox{} \\
      Iain Duff, 
      Jonathan Hogg, and Florent Lopez
     
\mbox{} \\
\end{center}

\vspace{1.0cm}


\noindent
{\large ABSTRACT}

\vspace{0.3cm}
\noindent
\theabstract

\vspace{0.6cm}

\begin{description}
\item [Keywords:] sparse Cholesky, SPD systems, runtime systems, STARPU, OpenMP
\item [AMS(MOS) subject classifications:]  65F30, 65F50
\end{description}

\vspace{0.1 cm}

\noindent \rule{15cm}{0.001in}
\vspace{0.1 cm}

\begin{description}

\item Scientific Computing Department, STFC Rutherford Appleton
  Laboratory, Harwell Campus,\\ Oxfordshire, OX11 0QX, UK.
\end{description}
\noindent
Correspondence to: florent.lopez@stfc.ac.uk\\
This work was supported by the FET-HPC H2020 NLAFET grant number xxxx.\\


\vspace{1.1 cm}
\noindent \today

\end{titlepage}

\newpage
\setcounter{page}{1}
\pagenumbering{arabic}


\lsection{Introduction} \label{sec:introduction} 

We investigate the use of a runtime system for the implementation of a
sparse Cholesky decomposition to solve the linear system
\begin{equation}\label{eq:linear-system}
  Ax = b,
\end{equation}
where $A$ is a large sparse symmetric positive-definite matrix. We
particularly focus on exploiting multicore architectures that are
now ubiquitous in high performance computing.
Despite their popularity, exploiting the capabilities of
multicore processors remains a challenge.
DAG-based algorithms have been shown to be extremely
efficient in terms of performance and scalability. They have
been widely used in the context of dense linear algebra such as in the
PLASMA software package~\cite{a.d.d.h.ea:09}
and have been adapted to
sparse algorithms in, for example, the \texttt{HSL\_MA87}
solver~\cite{h.r.s:10} that implements a supernodal
Cholesky factorization and the \texttt{qr\_mumps} solver~\cite{b:13}
that implements a multifrontal QR method.

The traditional approach for implementing a task-based solver includes
the development of an ad hoc scheduler which relies on the knowledge
of the algorithm 
to manage synchronisations and
enforce dependencies between processes and
is implemented using a low-level multithreading
library such as pthreads (POSIX threads) 
This approach, however, causes a
lack of portability of the implemented software as it is developed for
a given target architecture. It can then be costly in terms of
programming effort to port the code to different architectures and
adapt it to emerging ones. This problem occurs, for example, when
targeting heterogeneous architectures such as GPU-accelerated
multicore systems that have a more complex memory hierarchy than
traditional multicore systems and are equipped with  different types of
processing units with different capabilities. Instead, in this work,
we explore an alternative approach based on the use of a runtime
system that consists of a software layer between the architecture and
the application. In this context the application is implemented using
a high-level API provided by the runtime system, and low level details
such as data consistency and task scheduling
are delegated to the runtime system. Several dense linear algebra
software packages were built using this approach such as
DPLASMA~\cite{b.b.d.f.ea:11}, which uses the
\parsec~\cite{b.b.d.f.ea:13} runtime system and Chameleon which
supports several runtime systems including \starpu~\cite{a.t.n.w:11}
and \parsec. Both packages are designed for distributed memory systems
equipped with accelerators. In the context of sparse linear algebra,
however, relatively few libraries have adopted this approach. Two
examples of runtime-based sparse solvers are \qrm~\cite{a.b.g.l.:14},
that implements a multifrontal QR method, and \pastix~\cite{h.r.r:02}
that implements a supernodal Cholesky method. Compared to the context
of dense linear algebra, the difficulty with employing this approach in
the sparse case stems from the fact that the DAGs are extremely irregular
with a large variability of task granularity and irregularities in the
dependency pattern.

We focus on a Sequential Task Flow (STF) programming model for
expressing the DAG corresponding to our application. This model
offers a simple way to define the parallel code from
the sequential one. Although this work is closely related to the
approach employed in the \starpu version of \pastix, the expression of
dependencies differs between these two solvers. In the \pastix solver,
dependencies are explicitly expressed and therefore it does not take
advantage of the STF features available in \starpu. We show that our
codes, implemented with a task-based runtime system supporting the STF
model, can lead to the implementation of a sparse matrix factorization
that is as efficient as a state-of-the-art solver on a multicore
system.

%% advantage of GPU-accelerated arch? computation rate, energy
%% efficiency, etc

% computational intensity and device utilisation of kernels on the
% GPU

\lsection{Task-based sparse Cholesky factorization}\label{sec:chol}
\setcounter{equation}{0}
\setcounter{table}{0}
\setcounter{figure}{0}

We first describe the supernodal Cholesky factorization method that
used for solving sparse symmetric positive-definite systems. In
particular, we will focus on a DAG-based variant of this algorithm that
has been proven to be efficient on multicore architectures
in the \ma solver~\cite{h.r.s:10}. The factorization
is one of the three main phases for solving a linear system that
includes an analysis phase preceding the factorization, and a solve
phase following it. We specifically focus on the factorization because
it corresponds to the most computationally expensive phase.

The supernodal Cholesky method is a factorization algorithm for sparse
matrices where the input matrix $A$ is decomposed as
\begin{equation}\label{eq:chol}
  A = LL^{T},
\end{equation}
where the factor $L$ is a lower triangular matrix. The factorization
is then followed by a solve phase for computing $x$ through the solution
of the systems $Ly=b$ and $L^Tx=y$.
factor by means of forward and backward substitutions. These two steps
consist in solving first the system $Ly=x$, and second the system
$L^{T}x=y$. Note that the sparsity pattern of the factor is not the
same as the original matrix because additional coefficients
generally appear during the computation of this factor resulting in a
denser matrix than the original one. This phenomenon is called
\textit{fill-in} and can be greatly reduced by the use of a symmetric
permutation applied to the matrix before the factorization. If, for
example, we apply the symmetric permutation $P$ to the matrix $A$ then
we transform the original matrix into $PAP^{T}$. The two main
techniques for determining a fill-reducing symmetric permutation are
based on either Minimum Degree~\cite{t.w:67, l:85, a.d.d:96, a.d.d:04}
or Nested Dissection~\cite{g.l:78} methods. The dependencies between
the coefficients in the factor $L$ during the factorization can be
expressed in a tree structure called an \textit{elimination tree}
where each node in this tree represent a column in the factor. In
order to increase the efficiency of operations by exploiting Level 3
BLAS routines, the elimination tree is transformed into an
\textit{assembly tree} where columns having a similar nonzero pattern
are amalgamated into a dense matrix that is referred to as a
\textit{nodal} matrix or \textit{supernode}. 

The factorization is effected by traversing the assembly tree in a
topological order and performing two main operations at each
supernode: a dense Cholesky factorization of the current supernode and
an update of the ancestor supernodes using the resulting factors.
The update operations may be performed according
to several strategies: \textit{right-looking} updates where ancestors
nodes are updated as soon as the nodal factorization is done, or
\textit{left-looking} updates where the current supernode is updated
just before being factorized. The assembly tree is computed during the
analysis phase preceding the factorization and in our case we obtain
it using the software package HSL\_MC78~\cite{h.s:10}.

There are two main sources of parallelism that can be exploited in the
assembly tree: \textit{tree-level} and
\textit{node-level} parallelism. Tree-level parallelism is due to
the fact that supernodes located in separate branches of the tree can
be processed independently and node-level parallelism is exploited
when nodes are large enough to be efficiently processed
in parallel. One approach for the parallelization of the
supernodal algorithm consists in exploiting these two levels of
parallelism independently. For example, several processes can be used
to handle the factorization of independent supernodes in the tree and
these processes can exploit node-level parallelism by using
multithreaded routines. Note that, with this strategy, there is a
synchronisation point between the processing of a node and the
processing of its children therefore potentially limiting
parallelism. Instead, in our work, we follow the approach proposed
in~\cite{h.r.s:10} where supernodes are partitioned into square blocks
of order \texttt{nb} and operations are applied on these
blocks. Figure~\ref{fig:etree-simple-part} shows a simple assembly
tree that consists of three supernodes where the dashed lines
represent the block partitioning of
supernodes. Figure~\ref{fig:dag-simple} represents the DAG associated
with the factorization of the tree presented in
Figure~\ref{fig:etree-simple-part}.

\begin{figure}[!h]
  \begin{minipage}{0.5\textwidth}
    \centering
    \includegraphics[width=0.7\textwidth]{figures/etree_simple_part}
    \caption{\label{fig:etree-simple-part}Simple assembly tree with
      three supernodes partitioned into square blocks of order
      \texttt{nb}.}
  \end{minipage}
  \hspace{0.5cm}
  \begin{minipage}{0.5\textwidth}
    \centering
    \includegraphics[width=0.7\textwidth]{figures/dag}
    \caption{\label{fig:dag-simple} DAG corresponding to the
      factorization of the tree in Figure~\ref{fig:etree-simple-part}.}
  \end{minipage}
\end{figure}

As shown in the DAG represented in Figure~\ref{fig:dag-simple} they
are several kernels involved in the factorization of the supernodes:
the tasks denoted \texttt{f} correspond to the computation of the
Cholesky factor of a diagonal block. Tasks denoted \texttt{s} perform
a triangular solve of a subdiagonal block using a factor computed with
a task \texttt{f}. Tasks denoted \texttt{u} perform an update of a
block within a supernode corresponding to the previous factorization
of blocks. Finally, tasks denoted \texttt{a} represent the update
between supernodes with respect to the factorization blocks computed
in a given node.
  % For more details on the kernels, we refer
% to~\cite{h.r.s:10}.

In our code, the DAG, such as the one presented in
Figure~\ref{fig:dag-simple}, replaces the elimination tree for
expressing the dependencies during the computation of factors. Note
that when exploiting separately the node and tree parallelism, it is
not possible to start factorizing a supernode before all of its
descendant nodes have been processed. However, when using the DAG, it is
possible that some tasks in a given node become ready for execution
and are then scheduled while its descendants are still being
processed. Using this DAG-based parallelism it is therefore possible
to pipeline the processing of a given node with the processing of it
ancestors. This additional level of parallelism allowed by the use of
a DAG-based algorithm is referred to as inter-node parallelism.

\begin{figure}[!h]
  \centering \lstinputlisting{listings/spllt_facto_seq.f90}
\caption{\label{fig:spllt-facto-seq-pseudocode}Pseudo-code for the
  sequential version of the task-based sparse Cholesky factorization.}
\end{figure}

The pseudo-code corresponding to the task-based Cholesky factorization
is presented in Figure~\ref{fig:spllt-facto-seq-pseudocode}. Note that
this is the sequential algorithm that is used as a basis for the
implementation of parallel code. In this code we have the following
kernels:

\begin{itemize}
\item \texttt{alloc(snode)}: partition the supernode \texttt{snode}
  into blocks and allocate the data structures.
\item \texttt{init(snode)}: initializes the blocks by copying the
  coefficients from the original matrix into them.
\item \texttt{factorize(bc\_kk)}: compute the Cholesky factor of the
  diagonal block \texttt{bc\_kk}.
%% This is done with the \texttt{potrf} routine from LAPACK.
\item \texttt{solve(bc\_kk, bc\_ik)}: performs the triangular solve of
  an off-diagonal block \texttt{bc\_ik} with the block resulting from
  the factorization of the diagonal block \texttt{bc\_kk} in its
  column.
\item \texttt{update(bc\_ik, bc\_jk, bc\_ij)}: performs the update
  operation of a block \texttt{bc\_ij} within a supernode using the
  blocks \texttt{bc\_ik} and \texttt{bc\_jk} from a column previously
  processed.
\item \texttt{update\_btw(snode, bc\_ik, bc\_jk, anode, bc\_ij)}:
  performs the update operation of the block \texttt{bc\_ij} from the
  supernode \texttt{anode} with the blocks \texttt{bc\_ik} and
  \texttt{bc\_jk} from the descendant supernode \texttt{snode}.
\end{itemize}

In this algorithm, we perform the update using a
right-looking scheme. Although left and right looking schemes can
lead to different performance in serial mode, neither is
considered better because their behaviour depends on the
characteristics of the architecture.

\lsection{Sequential Task Flow parallel programming model}\label{sec:stf-model}

In this work we exploit a \textit{Sequential Task Flow
  (STF)} programming model for the implementation of a parallel
task-based Cholesky factorization on top of a runtime system. In this
model the detection of dependencies between tasks relies on a data
analysis of input and output data in order to guarantee the
\textit{sequential consistency} of operations during parallel
execution. This analysis is often referred to as
\textit{superscalar} analysis in deference to the dependency detection
between instructions that are performed in superscalar processors. In
this context, the dependency graph is used to allow the parallel
execution of independent instructions that is referred to as
instruction-level parallelism.
The STF model is the most commonly used paradigm for the
parallelization of DAG-based algorithms.
For example, several
dense linear algebra software packages such as
PLASMA~\cite{a.d.d.h.ea:09} and FLAME~\cite{i.c.q.q.ea:12} use this
model in their implementation. One reason for its popularity is
its ease of use: the parallel code is
very similar to the sequential one. Essentially, for a given
sequential algorithm, the
function calls (i.e. the execution of tasks in the case of a DAG-based
algorithm) are replaced by the asynchronous submission of the task to a runtime
system for scheduling. Depending on the data access
provided (read, write, or read/write), the runtime system automatically detects 
the dependencies
between the tasks. The sequential consistency is then ensured by the
fact that the order of submission of tasks corresponds to the
sequential order.

\begin{figure}[!h]
  \begin{minipage}{0.5\textwidth}
    \centering \lstset{language=C, procnamekeys={}}
    \lstinputlisting{listings/seq-example.c}
    \caption{\label{fig:seq-example}Simple example of a sequential code.}
    \vspace{0.5cm}
    \centering \lstinputlisting{listings/stf-example.c}
    \caption{\label{fig:stf-example}STF code corresponding to Figure~\ref{fig:seq-example} example.}
  \end{minipage}
  \hspace{0.5cm}
  \begin{minipage}{0.5\textwidth}
    \centering
    \includegraphics[width=0.7\textwidth]{figures/example_dag}
    \caption{\label{fig:dag-example}DAG corresponding to the
      sequential code presented in Figure~\ref{fig:stf-example}.}
  \end{minipage}
  %% \caption{\label{fig:seq-example} Simple example of a sequential code
  %%   on the left with the corresponding DAG represented on the right.}
\end{figure}

As an example, we consider the sequential code in
Figure~\ref{fig:seq-example} for which the corresponding DAG is
shown in Figure~\ref{fig:dag-example}. Based on a STF model, the
parallel version of this code is illustrated in
Figure~\ref{fig:stf-example}. In the sequential code, the two
functions \textit{f} and \textit{g} manipulate arrays \textit{x} and
\textit{y}. The STF code is obtain by submitting the tasks that
consist of a kernel funtion (\textit{f} or \textit{g} in this
example) together with data which are associated with a data access
which can be \textit{R} when the data is read, \textit{W} when the
data is written, and \textit{RW} when the data is read and modified.

Wile easy to sue, this model
has several drawbacks that may affect the performance and scalability
of parallel codes relying on it. The tasks are issued and submitted to
the runtime system sequentially. If the time to execute a task
is small compared to the time needed for building
and submitting a task, the parallel execution might be constrained
by the time spent in the submission loop.
To avoid this issue, a
\textit{recursive} model could be used where intermediate tasks
submit other tasks, enabling the parallelization of task submission.
This might be implemented, for example, using
\textit{callback} functions that are executed upon task completion to
trigger the submission of tasks depending on the task that just
finished its execution. Another issue arising with the STF model comes
from the fact that the whole DAG is unrolled during the parallel
execution and every task in the DAG is stored in order to track task
dependencies. In the case where the DAG is extremely large, handling
and storing the DAG might represent a large overhead in terms of
computational cost and memory storage. Even if the recursive model
allows us to mitigate the problem it doesn't remove it, and it may be
necessary to consider a different model such as the
Parametrized Task Graph (PTG) model introduced in~\cite{c.l:95}. In
this model, task dependencies are explicitly encoded with the dataflow
of each task and as a results the whole DAG can be expressed in a
compact format.

%% In the present paper we mainly focus on the 

\lsection{Runtime systems}\label{sec:runtime}
\setcounter{equation}{0}
\setcounter{table}{0}
\setcounter{figure}{0}

The popularity of task-based algorithms persuaded the OpenMP board to
introduce the \textit{task} construct in Version 3.0 of its API. Then,
motivated by the popularity of the STF model, the OpenMP board decided
to include the \textit{depend} construct in Version 4.0 allowing users
to express dependencies between tasks in a similar way to the STF
model presented in Section~\ref{sec:stf-model}. In this work we
use an OpenMP implementation of our Cholesky solver and show
advantages of using this in terms of performance, scalability and
productivity. However, because many features are still unavailable in
the OpenMP standard, we also developed a version based on the
StarPU runtime system. As shown in the next section, both
implementations of our solver rely on a STF model, but the
StarPU-based implementation can benefit from a wider range of features
available with StarPU. For example, although we focus on shared-memory
architectures in this work, the StarPU version can be extended to a
distributed-memory version whereas OpenMP doesn't provide any
possibility for execution on distributed memory architectures.

\begin{figure}[!h]
  \lstset{language=C, procnamekeys={},escapechar=>}
  \centering \lstinputlisting{listings/stf-openmp-example.c}
  \caption{\label{fig:stf-openmp-example}Simple example of a parallel version
    of the sequential code in Figure~\ref{fig:seq-example} using a STF
    model with \openmp.}
\end{figure}

We present an example of a parallel implementation for the sequential
code in Figure~\ref{fig:seq-example} using OpenMP in
Figure~\ref{fig:stf-openmp-example}. In this example we first create
the parallel section using the omp construct \textit{parallel} and
then we put the master thread in charge of the task submission using
the \textit{master} construct. As previously explained, the task are
created with the \textit{task} construct and the data access are given
to the runtime system using the \textit{depend} construct. In the
\openmp standard, read-only data access is indicated with the
parameter \textit{in}, write-only with the parameter \textit{out} and
read-write with the parameter \textit{inout}. Finally the task
submission loop is concluded with the \textit{taskwait} clause
indicating that the master thread should wait for the completion of
the tasks previously submitted.

Similarly to the \openmp example provided in
Figure~\ref{fig:stf-openmp-example}, and in order to introduce the
features provided by the StarPU API, we show in
Figure~\ref{fig:stf-starpu-example} an example of a StarPU-based
implementation for the simple example presented in
Figure~\ref{fig:seq-example}. The task submission is done through the
\texttt{starpu\_insert\_task} function that takes as input a
\textit{codelet} and a set of \textit{handles}. A codelet corresponds
to the description of a task and includes a list of computational
resources where the task can be executed as well as the corresponding
computational kernels. In our example the codelet \texttt{g\_cl} in
line~\ref{code:stf-starpu-example1} describes a task that can be
executed on a CPU and a CUDA device (\texttt{STARPU\_CPU |
  STARPU\_CUDA}) respectively with the kernels \texttt{g\_cpu\_func}
and \texttt{g\_cuda\_func}. The data handles, declared in
line~\ref{code:stf-starpu-example3} in our example, represent a piece
of data that is accessed in the task and can be read
(\texttt{STARPU\_R}), written (\texttt{STARPU\_W}), or read and
written (\texttt{STARPU\_RW}). In order to be used, a data handle must
be \textit{registered} to the runtime system by providing information
such as a pointer to the data, its size and type. This information
allows StarPU to automatically perform the data transfer between the
memory nodes during the execution. For example, when data needs to be
accessed on a GPU device, the runtime system automatically transfers it
to the device memory node. As a result, StarPU is capable of ensuring
data consistency over multiple nodes. When all the tasks have been
submitted to the runtime system, we wait for their completion by
calling the routine \texttt{starpu\_task\_wait\_for\_all}.

\begin{figure}[!h]
  \lstset{language=C, procnamekeys={},escapechar=>}
  \centering \lstinputlisting{listings/stf-starpu-example.c}
  \caption{\label{fig:stf-starpu-example}Simple example of a parallel version
    of the sequential code in Figure~\ref{fig:seq-example} using a STF
    model with \starpu.}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/scheduler}
    \caption{\label{fig:scheduler} Illustration of the dynamic
      scheduling strategy of tasks in the runtime system.}
\end{figure}

Both \openmp and \starpu implementations rely on a dynamic scheduler
performing the scheduling of the ready tasks during the execution. In
this model, a task is put in the scheduler as soon as it becomes ready
for execution, which is when all of its dependencies are
fulfilled. On the other side of the scheduler, workers try to retrieve
a task from the scheduler when they become idle. This dynamic
scheduling strategy is illustrated in Figure~\ref{fig:scheduler} where
the scheduler is placed between the runtime core where the DAG is
built as a result of the task submission and the workers which can be
CPUs and GPUs for example. The scheduler is responsible for storing
the ready tasks in scheduling queues and distributing them to idle
workers. Although \openmp Version 4.0 doesn't provide any features to
control the scheduling policy, Version 4.5 allows users to provide
a priority along with a submitted task so critical tasks are scheduled sooner.
\starpu not
only supports the use of task priorities but also makes it possible
to use different scheduling strategies and to implement a new one if
necessary.

\lsection{Parallelisation of a task-based Cholesky factorization using an STF programming model}
\label{sec:spllt-stf}

%% In this section we describe the STF-based parallel code of our
%% DAG-based Cholesky solver. We developed two different versions using
%% first the OpenMP standard and second the StarPU runtime system whose
%% features are presented in Section~\ref{sec:runtime}.  

%% Using the STF programming model introduced in
%% Section~\ref{sec:stf-model}, we show the implementation of the
%% supernodal Cholesky factorization in
%% Figure~\ref{fig:spllt-facto-pseudocode}.

In this section we describe the implementation of our DAG-based
Cholesky solver using the STF programming parallel model presented in
Section~\ref{sec:stf-model}. We developed two different versions of our
code; first the OpenMP standard, and secondly the StarPU runtime
system whose features were presented in Section~\ref{sec:runtime}.

A pseudo-code of our solver is presented in
Figure~\ref{fig:spllt-facto-pseudocode} and, following the sequential
algorithm shown in Figure~\ref{fig:spllt-facto-seq-pseudocode},
consists in a bottom-up traversal of the assembly tree where at each
node the tasks for the factorization and update operations are
submitted to the runtime system. The kernels used in the tasks, are
the same as those presented in Section~\ref{sec:chol}. Note that
the task submission is done using a right looking scheme meaning
that every node in the tree must be allocated and partitioned before
the submission of the numerical tasks. In addition, the \texttt{alloc}
task is executed sequentially because we need to allocate the data
structures and partition the supernodes in order to submit the
numerical tasks.

\begin{figure}[!h]
  \centering \lstinputlisting{listings/spllt_facto_stf.f90}
  \caption{\label{fig:spllt-facto-pseudocode}Pseudo-code for the sparse
    Cholesky factorization using a STF model presented in
    Section~\ref{sec:stf-model}.}
\end{figure}

As explained in Section~\ref{sec:stf-model}, when using the STF model
we need to provide the access mode along with the data when submitting
a task so that the runtime can ensure the sequential consistency of
the parallel algorithm. For this reason, in the submission of
\texttt{factorize} tasks, the diagonal block \texttt{blk(k,k)} is
associated with a read-write access mode indicating that the kernel is
going to read and modify this block as it computes the Cholesky factor
of this block. Similarly, as the \texttt{solve} operations require the
diagonal block for computing the factors in the subdiagonal blocks, we
have to indicate that this diagonal block is read when submitting the
\texttt{solve} by associating it with a read-only access mode. With
this information the runtime detects the dependencies between the
\texttt{factorize} and \texttt{solve} tasks and allows the parallel
execution of the solve tasks within a block-column. 

In order to make sure the supernode is initialized before the
factorization starts, we use a symbolic handle called \texttt{snode}
and pass it to the \texttt{init} tasks using a write access mode. Then
we also pass it to the \texttt{factorize} tasks in read access
mode. Because all the subsequent factorization tasks in a supernode
depend on the first \texttt{factorize} task, we thus guarantee that the
numerical task cannot start before the supernode is initialized. For
the same reason, the \texttt{update\_btw} task takes the
\texttt{anode} handle as input with read access mode because it
modifies a block in an ancestor node and the task should not be
executed before the node is initialized. The specific nature of this
symbolic handle is that it represents a set of blocks instead of a
single block.

One issue arises with the dependency detection of the \texttt{update}
tasks that are applied on a given block. This task takes as input two
blocks $L_{ik}$ and $L_{jk}$ and performs on a third block $L_{ij}$ the
operation
\begin{equation*}
  L_{ij} = L_{ij} - L_{ik}L_{jk}^{T}.
\end{equation*}
These update operations are commutative in infinite precision arithmetic. 
However, when two
\texttt{update} tasks applied to the same block are submitted, the
runtime system detects that these task modify the same data and will
enforce the same order of execution for these tasks as the order of
submission. With StarPU it is possible to avoid these unnecessary
dependency that potentially limit the parallelism using the
\texttt{STARPU\_COMMUTE} flag indicating that operations performed by
a kernel are commutative. The OpenMP standard still does not provide
such a functionality.

The STF code that is presented in
Figure~\ref{fig:spllt-facto-pseudocode} is independent from the
runtime system used for the implementation. In practice only the
implementation of the \texttt{submit} routines are specific to the
runtime system. This illustrates the fact that the expression of the
algorithm is strictly separated from the task scheduling and data
management. An example of the implementation of this
\texttt{submit} routine in the \starpu version is given in
Figure~\ref{fig:spllt-starpu-slv-task}, and its equivalent in the
\openmp version is given in Figure~\ref{fig:spllt-omp-slv-task}. In
this example we show the submission of the solve tasks. In the \openmp
version, blocks are identified using data pointers and these pointers
are associated with a data access when submitting a task. It is thus
necessary to allocate the blocks before being able to submit the tasks
that use these blocks. In the case of \starpu, blocks are associated
with a handle that is set up in the \texttt{alloc} routine. Tasks are
then associated with this handle instead of using a pointer as we do
with \openmp. There are several advantages associated with the use of
this handle. For example, \starpu is capable of detecting when data are
written for the first time and will allocate it using the information
contained in the handle. We don't use this feature for the allocation
of blocks, but we use it for the management of scratch memory that is
needed in the \texttt{update\_btw} task. We do not include this in the
pseudo-code for the sake of clarity.

\begin{figure}[!h]
  \begin{minipage}{0.5\textwidth}
    \centering
    \lstset{basicstyle=\tt\scriptsize, language=C}
    \lstinputlisting{listings/spllt_starpu_solve_task.c}
    \caption{\label{fig:spllt-starpu-slv-task} Submission routine used
      for the solve tasks in the \starpu code.}
  \end{minipage}
  \hspace{0.5cm}
  \begin{minipage}{0.5\textwidth}
    \centering
    \lstset{basicstyle=\tt\scriptsize}
    \lstinputlisting{listings/spllt_omp_solve_task.f90}
    \caption{\label{fig:spllt-omp-slv-task}Submission routine used
      for the solve tasks in the \openmp code.}
  \end{minipage}
\end{figure}

Note that the efficiency of these submission routines may be critical
to the performance of the execution and as shown in our tests, in some
cases, the submission of tasks in the DAG may be a limiting factor for
the performance. This happens when there is a large number of tasks
and the task granularity is small. In such cases, especially when the
number of resources increases, the unrolling of the DAG may be too
slow to feed all the resources therefore bounding the execution
time. In that respect, the partition parameter $nb$ may influence the
performance as a small value for this parameter increases the number
of tasks in the DAG and therefore the overhead associated with
task submission and task management in the runtime system
including the task scheduling.

%% The \texttt{init} task is responsible for setting up the blocks in a
%% given supernode. In our code a supernode is represented by a symbolic
%% data structure denoted \texttt{snode} and we indicate that we modify
%% this structure by using the \textit{write} data-access in the
%% \texttt{init} task. This \texttt{snode} symbolic structure represent
%% the information associated with a supernode as well as the blocks
%% associated with the nodal matrix.

\alert{Should we introduce another section to present the MA87
  scheduler and task dependency management?}

\lsection{Experimental results}\label{sec:experiments}

In this section we present the experimental results obtained on our
test matrices taken from the University of Florida Matrix
Collection~\cite{d.h:11}. In this collection we selected a large set
of symmetric positive-definite matrices from a wide range of
application and sparsity structure. They are listed in
Table~\ref{table:mat} along with their sizes and number of entries. In
this table we also indicated the number of entries in the factor $L$
and the operation count for the factorization when using the
nested-dissection ordering \metis~\cite{k.k:98}. As explained in
Section~\ref{sec:chol} this allows us to reduce the number of entries
in the factor $L$ by limiting the fill-in. Note that in this table,
matrix characteristics are obtained without node amalgamation. This
means that the number of entries in $L$ as well as the flop count is
minimized. However, in our experiments we use node amalgamation to
obtain a better efficiency of operations at the cost of an increase in
the number of entries in $L$ as well as the operation count. The node
amalgamation is controlled by a parameter \texttt{nemin} used during
the analysis phase and the strategy for amalgamating the nodes in
analysis routine of the MA78 is as follow: the elimination tree is
traversed using a post order and when a node is visited, it is merge
with its parents if the column count in both nodes is lower than
\texttt{nemin} or if the merging generate no additional fill-in in
$L$. In our experiment we set the \texttt{nemin} value to $32$ which
correspond to a good trade-off between sparsity and efficiency of
operations in our experiments.

In this study the tests were made on a multicore machine equipped with
two Intel(R) Xeon(R) E5-2695 v3 CPUs with fourteen cores each (twenty
eight cores in total). Each core, clocked at 2.3 Ghz and is equipped
with AVX2, has a peak of 36.8 GFlop/s corresponding to a total peak of
1.03 TFlop/s in real, double precision arithmetic. The code is
compiled with the GNU compiler (\texttt{gcc} and \texttt{gfortran}),
the BLAS and LAPACK routines are provided by the Intel MKL v11.3
library and we used the trunk development version of the \starpu
runtime system freely available through anonymous SVN access.

The choice for the parameter \nb in the parallel execution is not
trivial as it impacts several aspects of the execution that influences
the performance. Although small value for this parameter increases the
number of tasks in the DAG and thus the parallelism, it also reduces
the performance of the level 3 BLAS routines executed in the
tasks. The optimal value for this parameter thus correspond to a
trade-off between a sufficient amount of parallelism to feed the
resources and a good kernels efficiency. In addition, as we have seen
in Section~\ref{sec:spllt-stf} the parameter \nb influence the
overhead for managing the task because when the number of tasks in the
DAG increases, it also increase the time needed for the handling the
DAG including the task submission, dependency detection and
scheduling. Finally, the optimal value for \nb depend on a large
number of parameters such as processing unit capabilities, number of
resources and cannot be easily determined without a precise
performance model for the application which is, in practice, extremely
difficult to establish. Instead we empirically determine a good \nb
value for each problem by running multiple tests on a range of values
which is \texttt{nb = (256, 384, 512, 768, 1024)} in these experiments.

The best factorization times obtained with \spllt and \ma for every
problem listed in Table~\ref{table:mat} are reported in
Table~\ref{table:exp-times} along with the corresponding value of \nb
for the experiment. The Gflop/s rates corresponding to the best
factorization times are represented in Figures~\ref{fig:exp-perf} for
the different solvers. Additionally Figures~\ref{fig:exp-rel-perf}
illustrate the relative performance of the \spllt codes compared \ma.

For matrices from \#27 to \#38, corresponding to the bigger problems
of our test set, the performance behaviour of the \openmp and \starpu
version of \spllt is very similar and comparable to the one obtained
with our reference solver \ma. On smaller problems, with factorization
times generally smaller than a second, it appears that the \openmp
version gives better results that the \starpu one, with results
competitive with \ma except on some specific problems. Matrices \# 1
and \# 15, for examples, gives extremely poor results with both
version of our code.

%% \begin{figure}[!h]
%%   \begin{minipage}{0.5\textwidth}
%%     \centering
%%     \includegraphics[width=\textwidth]{data/cmp_perf_stf}
%%     \caption{\label{fig:exp-perf}\TODO{put caption}}
%%   \end{minipage}
%%   \hspace{0.5cm}
%%   \begin{minipage}{0.5\textwidth}
%%     \centering
%%     \includegraphics[width=\textwidth]{data/cmp_facto_rel_stf}
%%     \caption{\label{fig:exp-perf}\TODO{put caption}}
%%   \end{minipage}
%% \end{figure}

\begin{table}[htbp]
    \begin{center}
      \texttt{ \input{data/cn255/table_cmp_facto.tex}}
    \end{center}
    \caption{\label{table:exp-times}Factorization times (seconds) obtained with MA87 and
      SpLLT (i.e. MA87\_starpu). The factorizations were run with the
      block sizes \texttt{nb=(256, 384, 512, 768, 1024)} on 28 cores
      and \texttt{nemin=32}. The lowest factorization times are shown
      in bold.}
\end{table}

%\newpage

\begin{figure}[!h]
  \centering
  \includegraphics[width=\textwidth]{data/cmp_perf_stf}
  \caption{\label{fig:exp-perf} Performance results for both \openmp
    and \starpu versions of SpLLT solver and \ma solver on 28 cores
    for our test matrices presented in Table~\ref{table:mat}.}
\end{figure}

\begin{figure}[!h]
  \centering
  \includegraphics[width=\textwidth]{data/cmp_facto_rel_stf}
  \caption{\label{fig:exp-rel-perf}Performance results obtained with
    SpLLT, \openmp and \starpu versions, relatively to the performance
    obtained with \ma.}
\end{figure}

\section{Concluding remarks}\label{sec:conclusions}
This report has described in detail the development of a new

 
%% \section*{Code Availability}
%% A development version of the Cholesky factorization software used in this 
%% paper may be checked out of
%% our source code repository using the following command:

%% \begin{verbatim}
%%    svn co -r612 http://ccpforge.cse.rl.ac.uk/svn/spral/branches/xxxxxxx
%% \end{verbatim}

%% This code has not yet been optimised and so is not yet
%% part of the HSL or SPRAL libraries that are we develop
%% and maintain at the Rutherford Appleton Laboratory (see
%% \url{http://www.hsl.rl.ac.uk/} and \url{http://www.numerical.rl.ac.uk/spral/}).

\clearpage

\bibliography{flipflapflopBib}

\clearpage

\appendix

\section{Test problems}\label{appendix}
\setcounter{equation}{0}
\setcounter{table}{0}
\setcounter{figure}{0}
\setcounter{table}{0}
\renewcommand{\thetable}{A.\arabic{table}}

\begin{table}[htbp]
  \label{table:mat}
  \begin{center}
    \texttt{\begin{tabular}{rl|rrrrl}
      \hline
      \# & Name                            & n        & nz(A)    & nz(L)    & Flops    & Application/Description  \\
         &                                 & ($10^3$) & ($10^6$) & ($10^6$) & ($10^9$) &                          \\
      \hline
      1  & Schmid/thermal2                 & 1228     & 4.9      & 51.6     & 14.6     & Unstructured thermal FEM \\
      2  & Rothberg/gearbox                & 154      & 4.6      & 37.1     & 20.6     & Aircraft flap actuator   \\
      3  & DNVS/m\_t1                      & 97.6     & 4.9      & 34.2     & 21.9     & Tubular joint            \\
      4  & Boeing/pwtk                     & 218      & 5.9      & 48.6     & 22.4     & Pressurised wind tunnel  \\
      5  & Chen/pkustk13                   & 94.9     & 3.4      & 30.4     & 25.9     & Machine element          \\
      6  & GHS\_psdef/crankseg\_1          & 52.8     & 5.3      & 33.4     & 32.3     & Linear static analysis   \\
      7  & Rothberg/cfd2                   & 123      & 1.6      & 38.3     & 32.7     & CFD pressure matrix      \\
      8  & DNVS/thread                     & 29.7     & 2.2      & 24.1     & 34.9     & Threaded connector       \\
      9  & DNVS/shipsec8                   & 115      & 3.4      & 35.9     & 38.1     & Ship section             \\
      10 & DNVS/shipsec1                   & 141      & 4.0      & 39.4     & 38.1     & Ship section             \\
      11 & GHS\_psdef/crankseg\_2          & 63.8     & 7.1      & 43.8     & 46.7     & Linear static analysis   \\
      12 & DNVS/fcondp2                    & 202      & 5.7      & 52.0     & 48.2     & Oil production platform  \\
      13 & Schenk\_AFE/af\_shell3          & 505      & 9.0      & 93.6     & 52.2     & Sheet metal forming      \\
      14 & DNVS/troll                      & 214      & 6.1      & 64.2     & 55.9     & Structural analysis      \\
      15 & AMD/G3\_circuit                 & 1586     & 4.6      & 97.8     & 57.0     & Circuit simulation       \\
      16 & GHS\_psdef/bmwcra\_1            & 149      & 5.4      & 69.8     & 60.8     & Automotive crankshaft    \\
      17 & DNVS/halfb                      & 225      & 6.3      & 65.9     & 70.4     & Half-breadth barge       \\
      18 & Um/2cubes\_sphere               & 102      & 0.9      & 45.0     & 74.9     & Electromagnetics         \\
      19 & GHS\_psdef/ldoor                & 952      & 23.7     & 144.6    & 78.3     & Large door               \\
      20 & DNVS/ship\_003                  & 122      & 4.1      & 60.2     & 81.0     & Ship structure           \\
      21 & DNVS/fullb                      & 199      & 6.0      & 74.5     & 100.2    & Full-breadth barge       \\
      22 & GHS\_psdef/inline\_1            & 504      & 18.7     & 172.9    & 144.4    & Inline skater            \\
      23 & Chen/pkustk14                   & 152      & 7.5      & 106.8    & 146.4    & Tall building            \\
      24 & GHS\_psdef/apache2              & 715      & 2.8      & 134.7    & 174.3    & 3D structural problem    \\
      25 & Koutsovasilis/F1                & 344      & 13.6     & 173.7    & 218.8    & AUDI engine crankshaft   \\
      26 & Oberwolfach/boneS10             & 915      & 28.2     & 278.0    & 281.6    & Bone micro-FEM           \\
      27 & ND/nd12k                        & 36.0     & 7.1      & 116.5    & 505.0    & 3D mesh problem          \\
      28 & JGD\_Trefethen/Trefethen\_20000 & 20.0     & 0.3      & 90.7     & 652.6    & Integer matrix           \\
      29 & ND/nd24k                        & 72.0     & 14.4     & 321.6    & 2054.4   & 3D mesh problem          \\
      30 & Janna/Flan\_1565                & 1565     & 59.5     & 1477.9   & 3859.8   & 3D mechanical problem    \\
      31 & Oberwolfach/bone010             & 987      & 36.3     & 1076.4   & 3876.2   & Bone micro-FEM           \\
      32 & Janna/StocF-1465                & 1465     & 11.2     & 1126.1   & 4386.6   & Underground aquifer      \\
      33 & GHS\_psdef/audikw\_1            & 944      & 39.3     & 1242.3   & 5804.1   & Automotive crankshaft    \\
      34 & Janna/Fault\_639                & 639      & 14.6     & 1144.7   & 8283.9   & Gas reservoir            \\
      35 & Janna/Hook\_1498                & 1498     & 31.2     & 1532.9   & 8891.3   & Steel hook               \\
      36 & Janna/Emilia\_923               & 923      & 21.0     & 1729.9   & 13661.1  & Gas reservoir            \\
      37 & Janna/Geo\_1438                 & 1438     & 32.3     & 2467.4   & 18058.1  & Underground deformation  \\
      38 & Janna/Serena                    & 1391     & 33.0     & 2761.7   & 30048.9  & Gas reservoir            \\
      \hline
    \end{tabular}}
  \end{center}
  \caption{Test matrices and their characteristics without node
    amalgamation. $n$ is the matrix order, $nz(A)$ represent the
    number entries in the matrix $A$, $nz(L)$ represent the number of
    entries the factor $L$ and \textit{Flops} correspond to the operation
    count for the matrix factorization.}
\end{table}

%% In Table~\ref{Tbl:Problems} we list  our test problems along with 
%% their characteristics. The problems are from the 
%% University of Florida Sparse Matrix Collection  and are chosen 
%% to represent a wide range of sparsity structures.

\end{document}
