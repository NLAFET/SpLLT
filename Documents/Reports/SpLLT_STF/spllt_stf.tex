\documentclass{article}
\pagenumbering{arabic}

\usepackage{url}
\usepackage{color}
\usepackage{verbatim}
\usepackage{tikz}
\usepackage{gnuplot-lua-tikz}

\usepackage{xspace}

%\usepackage[utf8]{inputenc}
%\usepackage{fontspec}
\usepackage{pgfplots}

\usepackage[procnames]{listings}
\lstset{ %
  backgroundcolor=\color{gray98},    % choose the background color; you must add \usepackage{color} or \usepackage{xcolor}
  basicstyle=\tt\small, % \prettysmall      % the size of the fonts that are used for the code
  breakatwhitespace=false,          % sets if automatic breaks should only happen at whitespace
  breaklines=true,                  % sets automatic line breaking
  showlines=true,                  % sets automatic line breaking
  captionpos=b,                     % sets the caption-position to bottom
  commentstyle=\color{gray30},      % comment style
  extendedchars=true,               % lets you use non-ASCII characters; for 8-bits encodings only, does not work with UTF-8
  frame=single,                     % adds a frame around the code
  keepspaces=true,                  % keeps spaces in text, useful for keeping indentation of code (possibly needs columns=flexible)
  keywordstyle=\color{amblu},       % keyword style
  procnamestyle=\color{amred},       % procedures style
  language=[95]fortran,             % the language of the code
  numbers=left,                     % where to put the line-numbers; possible values are (none, left, right)
  numbersep=5pt,                    % how far the line-numbers are from the code
  numberstyle=\tiny\color{gray20}, % the style that is used for the line-numbers
  rulecolor=\color{gray20},          % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. comments (green here))
  showspaces=false,                 % show spaces everywhere adding particular underscores; it overrides 'showstringspaces'
  showstringspaces=false,           % underline spaces within strings only
  showtabs=false,                   % show tabs within strings adding particular underscores
  stepnumber=2,                     % the step between two line-numbers. If it's 1, each line will be numbered
  stringstyle=\color{amblu},       % string literal style
  tabsize=2,                        % sets default tabsize to 2 spaces
  % title=\lstname,                    % show the filename of files included with \lstinputlisting; also try caption instead of title
  procnamekeys={call}
}

\usepackage{color}
\definecolor{gray98}{rgb}{0.98,0.98,0.98}
\definecolor{gray20}{rgb}{0.20,0.20,0.20}
\definecolor{gray25}{rgb}{0.25,0.25,0.25}
\definecolor{gray16}{rgb}{0.161,0.161,0.161}
\definecolor{gray60}{rgb}{0.6,0.6,0.6}
\definecolor{gray30}{rgb}{0.3,0.3,0.3}
\definecolor{bgray}{RGB}{248, 248, 248}
\definecolor{amgreen}{RGB}{77, 175, 74}
% \definecolor{amblu}{RGB}{72, 88, 102}
\definecolor{amblu}{RGB}{55, 126, 184}
\definecolor{amred}{RGB}{228,26,28}
\definecolor{amyellow}{RGB}{237,177,32}
\definecolor{ampurple}{RGB}{126,47,142}
\newcommand{\mye}[1]{\textcolor{amyellow}{#1}\xspace}
\newcommand{\mgr}[1]{\textcolor{amgreen}{#1}\xspace}
\newcommand{\mbl}[1]{\textcolor{amblu}{#1}\xspace}
\newcommand{\mre}[1]{\textcolor{amred}{#1}\xspace}
\newcommand{\mbk}[1]{\textcolor{black}{#1}\xspace}
\newcommand{\mbp}[1]{\textcolor{ampurple}{#1}\xspace}

\input{nlafet_style.sty}

\newcommand{\starpu}{{StarPU}\xspace}
\newcommand{\parsec}{{PaRSEC}\xspace}
\newcommand{\TODO}{\alert{TODO}\xspace}
\newcommand{\openmp}{OpenMP\xspace}
\newcommand{\ma}{HSL\_MA87\xspace}
\newcommand{\spllt}{SpLLT\xspace}

\bibliographystyle{siam}

%-----------------------------------------------------------------------
%
% include macros
%
\input nlafet_macros.tex
%-----------------------------------------------------------------------



\newcommand{\stfccovertitle}
{Experiments with sparse Cholesky using runtime systems}


\newcommand{\theabstract}{We describe the development of a prototype code for 
the solution of large symmetric positive definite sparse systems that is
efficient on parallel architectures.
}

\textwidth  16.18cm
\textheight 23.4cm
\oddsidemargin -0.2mm
\evensidemargin -0.2mm
\def\baselinestretch{1.1}
\topmargin -8.4mm

\newcommand{\n}[1][1]{\vspace*{#1\baselineskip}\noindent}
\newcommand{\ISD}[1]{\begin{center}\n[.4]\textcolor{red}{ISD\
 \hspace*{0.15em} \fbox{\parbox{.8\textwidth}{#1}}}\n[.4]\end{center}}
\newcommand{\HSD}[1]{\begin{center}\n[.4]\textcolor{green}{FL\
\hspace*{0.15em} \fbox{\parbox{.9\textwidth}{\texttt{#1}}}}\n[.4]\end{center}}
\newcommand{\ASH}[1]{\begin{center}\n[.4]\textcolor{brown}{JH\
\hspace*{0.15em} \fbox{\parbox{.9\textwidth}{\texttt{#1}}}}\n[.4]\end{center}}

\newcommand{\metis}{{\sc Me$\!$T$\!$iS\ }}

\hyphenpenalty=10000
\widowpenalty=10000
\sloppy

\begin{document}


\begin{titlepage}

\vspace*{-0.5cm}

\vspace{1.0 cm}

{\Large \bf
\begin{center}
   \stfccovertitle
\end{center}}

\begin{center}
\mbox{} \\
      Iain Duff\footnotemark[1], 
      Jonathan Hogg\footnotemark[1], and Florent Lopez\footnotemark[1]
     
\mbox{} \\
\end{center}

\vspace{1.0cm}


\noindent
{\large ABSTRACT}

\vspace{0.3cm}
\noindent
\theabstract

\vspace{0.6cm}

\begin{description}
\item [Keywords:] sparse Cholesky, SPD systems, runtime systems, STARPU, OpenMP
\item [AMS(MOS) subject classifications:]  65F30, 65F50
\end{description}

\vspace{0.1 cm}

\noindent \rule{15cm}{0.001in}
\vspace{0.1 cm}

\begin{description}

\item [$^1$] Scientific Computing Department, STFC Rutherford 
Appleton Laboratory,
Harwell Campus,\\ Oxfordshire, OX11 0QX, UK.
\end{description}
\noindent
Correspondence to: florent.lopez@stfc.ac.uk\\
This work was supported by the FET-HPC H2020 NLAFET grant number xxxx.\\


\vspace{1.1 cm}
\noindent \today

\end{titlepage}

\newpage
\setcounter{page}{1}
\pagenumbering{arabic}

\section{Introduction} \label{sec:introduction} 



In this work we investigate the use of a runtime system for the
implementation of a sparse Cholesky solver in order to find the
solution of the linear system:

\begin{equation}\label{eq:ch}
  Ax = b
\end{equation}

where $A$ is a large sparse symmetric positive-definite matrix. We
particularly focus on exploiting multicore architectures that have
become increasingly popular since their introduction and can be found
nowadays in the vast majority of high performance computing
platforms. Despite their popularity, exploiting the capabilities of
multicore processors remain a challenge and to cope with multicore
hardware, DAG-based algorithms have been shown to be extremely
efficient in terms of performance and scalability. Initially they have
been employed in the context of dense linear algebra such as in the
PLASMA~\cite{a.d.d.h.ea:09} software package and motivated by the high
efficiency provided by these algorithms, they have been adapted to
sparse algorithms with the example of the \texttt{HSL\_MA87}
solver~\cite{h.r.s:10} that implements a DAG-based sparse Cholesky
factorization and the \texttt{qr\_mumps} solver~\cite{b:13} implements
a DAG-based multifrontal QR method.

The traditional approach for implementing a task-based solver includes
the development of an adhoc scheduler which relies on the knowledge of
the algorithm and is implemented using a low-level multithreading
library such as pthread (POSIX threads) to manage synchronisations and
enforce dependencies between processes. This approach however induces
a lack of portability of the implemented software as it is bound to a
given target architecture. It is then costly in terms of programming
effort to port the code to different architectures and adapt to the
emerging ones. This problem occurs for example when targeting
heterogeneous architectures such as GPU-accelerated system that have a
more complex memory hierarchy than multicore systems and different
types of processing units with different capabilities. In this work,
instead, we explore an alternative approach based on the use of a
runtime system which consists in a software layer between the
architecture and the application. In this context the application is
then implemented using a high-level API provided by the runtime system
and low level details such as data consistency across the architecture
and task scheduling are delegated to the runtime system. In this work,
we focus on the exploitation of a Sequential Task Flow (STF)
programming model for expressing the DAG corresponding to our
application. In this work, we show that our code, implemented with a
task-based runtime system, can lead to the implementation a sparse
matrix factorization which is as efficient as a state-of-the-art
solver on a multicore system.

\section{Task-based sparse Cholesky factorization}\label{sec:chol}
\setcounter{equation}{0}
\setcounter{table}{0}
\setcounter{figure}{0}

In this section we describe the supernodal Cholesky factorization
method that we use in this work for solving sparse symmetric
positive-definite systems. In particular we will focus on a DAG-based
variant of this algorithm that has been proven to be extremely
efficient on multicore architectures such as in~\cite{h.r.s:10} with
the \ma solver.

The supernodal Cholesky method is a factorization algorithm for sparse
matrices where the input matrix $A$ is decomposed as following:

\begin{equation}\label{eq:ch}
  A = LL^{T}
\end{equation}

where the factor $L$ is a lower triangular matrix. The dependencies
between the coefficients in the factor $L$ during the factorization
can be expressed in a tree structure called \textit{elimination tree}
where each node in this tree represent a column in the factor. In
order to increase the efficiency of operations by exploiting Level-3
BLAS routines, the elimination tree is transformed into an
\textit{assembly tree} where columns having a similar nonzero pattern
are amalgamated into a dense matrix that is referred to as
\textit{nodal} matrix or \textit{supernode}. The factorization is
effected by traversing the assembly tree in a topological order and
performing two main operations at each supernode: a dense Cholesky
factorization of the current supernode and update the ancestor
supernodes with respect to the nodal factorization. The update
operations may be performed according to several strategies such:
\textit{right-looking} updates where ancestors nodes are updated as
soon as the nodal factorization is done, or, \textit{left-looking}
updates where the current supernode is updated just before being
factorized. The assembly tree is computed during an analysis phase
preceding the factorization and in our case we obtain it by using the
software package HSL\_MC78.

There are two main sources of parallelism that can be exploited in the
assembly tree which are the \textit{tree-level} and
\textit{node-level}. The tree-level parallelism is due to the fact
that supernode located in separate branches in the tree can be
processed independently and the node-level parallelism is exploited
when nodes are large enough and can be processed by multiple resources
in parallel. One approach for the parallelization of the supernodal
consists in exploiting separately these two levels of parallelism. In
our work, instead, we follows the approach proposed in~\cite{h.r.s:10}
where supernodes are partitioned into square blocks of order
\texttt{nb} and operations are applied on these
blocks. Figure~\ref{fig:etree-simple-part} shows a simple assembly
tree that consists of three supernodes where the dashed lines
represent the block partitioning of
supernodes. Figures~\ref{fig:dag-simple} represent the DAG associated
with the factorization of the tree presented in
Figure~\ref{fig:etree-simple-part}.

\begin{figure}[!h]
  \begin{minipage}{0.5\textwidth}
    \centering
    \includegraphics[width=0.7\textwidth]{figures/etree_simple_part}
    \caption{\label{fig:etree-simple-part}Simple assembly tree with
      three supernodes partitioned into square blocks of order
      \texttt{nb}.}
  \end{minipage}
  \hspace{0.5cm}
  \begin{minipage}{0.5\textwidth}
    \centering
    \includegraphics[width=0.7\textwidth]{figures/dag}
    \caption{\label{fig:dag-simple} DAG corresponding to the
      factorization of the tree in Figure~\ref{fig:etree-simple-part}.}
  \end{minipage}
\end{figure}

As shown in the DAG represented in Figure~\ref{fig:dag-simple} they
are several kernels involved in the factorization of the supernodes:
the tasks denoted \texttt{f} correspond to the computation of the
Cholesky factor of a diagonal block. Tasks denoted \texttt{s} perform
a triangular solve of a subdiagonal block using a factor computed with
a task \texttt{f}. Tasks denoted \texttt{u} perform an update of a
block within a supernode with respect to the previous factorization of
blocks. Finally, tasks denoted \texttt{a} represent the update between
supernodes with respect to the factorization blocks computed in a
given node. 
  % For more details on the kernels, we refer
% to~\cite{h.r.s:10}.

In our code, the DAG, such as the one presented in
Figure~\ref{fig:dag-simple}, replaces the elimination tree for
expressing the dependencies during the computation of factors. Note
that when exploiting separately the node and tree parallelism, it is
not possible to start factorizing a supernode before all of it
descendent have been processed. However, when relying on the DAG, it
is possible that some tasks in a given node become ready for execution
while its descendent are being processed. Using this DAG-based
parallelism it is therefore possible to pipeline the processing of a
given node with the processing of it ancestors. This additional level
of parallelism allowed by the use of a DAG-based algorithm is referred
to as inter-node parallelism.
 
\section{Sequential Task Flow parallel programming model}\label{sec:runtime}

In this work we propose exploiting a \textit{Sequential Task Flow
  (STF)} programming model for the implementation of a parallel
task-based Cholesky factorization on top of a runtime system. In this
model the detection of dependencies between tasks relies on a data
analysis of input and output data in order to guarantee the
\textit{sequential consistency} of operations during a parallel
execution. This analysis is often referred to as a
\textit{superscalar} analysis in reference to the dependency detection
between instructions that are performed in superscalar processors. In
this context the dependency graph is used to allow the parallel
execution of independent instructions which is referred to as
instruction-level parallelism and increases the instruction
throughput. The STF model is the most commonly used paradigm for the
parallelization of DAG-based algorithms that have become more and more
popular in the scientific computing community. For example, several
dense linear algebra software packages such as
PLASMA~\cite{a.d.d.h.ea:09} and FLAME~\cite{i.c.q.q.ea:12} use this
paradigm in their implementation. One reason for such a popularity is
that it is extremely easy to use this model as the parallel code is
very similar to the sequential one. Essentially, for a given
sequential algorithm, the STF code is implemented by replacing the
function calls (i.e. the execution of tasks in the case of a DAG-based
algorithm) with the asynchronous submission of this task to a runtime
system in charge of the scheduling. Depending on the data access
provided, the runtime system automatically detects the dependencies
between the tasks. The sequential consistency is then ensured by the
fact that the order of submission of tasks corresponds to the
sequential order.

\begin{figure}[!h]
  \begin{minipage}{0.5\textwidth}
    \centering \lstset{language=C, procnamekeys={}}
    \lstinputlisting{listings/seq-example.c}
    \caption{\label{fig:seq-example}Simple of a sequential code}
    \vspace{0.5cm}
    \centering \lstinputlisting{listings/stf-example.c}
    \caption{\label{fig:stf-example}STF code}
  \end{minipage}
  \hspace{0.5cm}
  \begin{minipage}{0.5\textwidth}
    \centering
    \includegraphics[width=0.7\textwidth]{figures/example_dag}
    \caption{\label{fig:dag-example}DAG corresponding to the
      sequential code presented in Figure~\ref{fig:stf-example}}
  \end{minipage}
  %% \caption{\label{fig:seq-example} Simple example of a sequential code
  %%   on the left with the corresponding DAG represented on the right.}
\end{figure}

As an example, we consider the sequential code in
Figure~\ref{fig:seq-example} for which the corresponding DAG is
represented in Figure~\ref{fig:dag-example}. Based on a STF model, the
parallel version of this code is illustrated in
Figure~\ref{fig:stf-example}. In the sequential code, the two
functions \textit{f} and \textit{g} manipulate arrays \textit{x} and
\textit{y}. The STF code is obtain by submitting the tasks that
consist of a kernels funtion (\textit{f} or \textit{g} in this
example) together with data which are associated with a data access
which can be \textit{R} when the data is read, \textit{W} when the
data is modified, and \textit{RW} when the data is read and modified.

As we have seen with this simple example, the STF model is extremely
easy to use and this characteristic is certainly one of the main
reasons for its popularity in the HPC community. This model, however,
has several drawbacks that may affect the performance and scalability
of parallel codes relying on it. The task are issued and submitted to
the runtime system sequentially. In the case where, for a given DAG,
the task granularity is small compared to the time needed for building
and submitting a task, the parallel execution might be constrained
by the time spent in the submission loop that is setting up the
DAG. To avoid this issue, it might be interesting to consider a
\textit{recursive} model where intermediate tasks might be used to
submit other tasks enabling the distribution of the cost for building
the DAG between the resources instead of doing it in a single
submission loop. This might be implemented for example using
\textit{callback} functions that are executed upon task completion and
can trigger the submission of tasks depending on the task that just
finished its execution. Another issue arising with the STF model comes
from the fact that the whole DAG is unrolled during the parallel
execution and every task in the DAG is stored in order to track task
dependencies. In the case where the DAG is extremely large, handling
and storing the DAG might represent an important overhead in terms of
computational cost and memory storage. Even if the recursive model
allows us to mitigate the problem it doesn't remove it, and it may be
necessary to consider a radically different model such as the
Parametrized Task Graph (PTG) model introduced in~\cite{c.l:95}. In
this model, task dependencies are explicitly encoded with the dataflow
of each task and as a results the whole DAG can be expressed in a
compact format.

%% In the present paper we mainly focus on the 

\section{Runtime systems}\label{sec:runtime}
\setcounter{equation}{0}
\setcounter{table}{0}
\setcounter{figure}{0}

The popularity of task-based algorithms persuaded the OpenMP board to
introduce the \textit{task} construct in Version 3.0 its API. Then
motivated by the popularity of the STF model, the OpenMP board decided
to include the \textit{depend} construct in Version 4.0 allowing users
to express dependencies between tasks in a similar way to the STF
model. In this work we propose an OpenMP implementation of our
Cholesky solver and show advantages of using this in terms of
performance, scalability and productivity. However, because many
features are still unavailable in the OpenMP standard we also
developed another version based on the StarPU runtime system. Both
implementations of our solver rely on a STF model, but the
StarPU-based implementation can benefit from a wider range of features
available with StarPU. For example, although we focus on shared-memory
architectures in this work, the StarPU version might be extended to a
distributed-memory version whereas OpenMP doesn't provide any
possibility for execution on distributed memory architectures.

We present an example of a parallel implementation for the sequential
code in Figure~\ref{fig:seq-example} using OpenMP in
Figure~\ref{fig:stf-openmp-example}.

\begin{figure}[!h]
  \lstset{language=C, procnamekeys={},escapechar=>}
  \centering \lstinputlisting{listings/stf-openmp-example.c}
  \caption{\label{fig:stf-openmp-example}Simple example of a parallel version
    of the sequential code in Figure~\ref{fig:seq-example} using a STF
    model with \openmp.}
\end{figure}

In order to introduce the features provided by the StarPU API, we show
in Figure~\ref{fig:stf-starpu-example} an example of a StarPU-based
implementation for the simple example presented in
Figure~\ref{fig:seq-example}. The task submission is done through the
\texttt{starpu\_insert\_task} function that takes as input a
\textit{codelet} and a set of \textit{handles}. A codelet corresponds
to the description of a task and includes a list of computational
resources where the task can be executed as well as the corresponding
computational kernels. In our example the codelet \texttt{g\_cl} in
line~\ref{code:stf-starpu-example1} describes a task that can be
executed on a CPU and a CUDA device (\texttt{STARPU\_CPU |
  STARPU\_CUDA}) respectively with the kernels \texttt{g\_cpu\_func}
and \texttt{g\_cuda\_func}. The data handles, declared in
line~\ref{code:stf-starpu-example3} in our example, represent a piece
of data that is accessed in the task and can be read
(\texttt{STARPU\_R}), written (\texttt{STARPU\_W}), or read and
written (\texttt{STARPU\_RW}). In order to be used, a data handle must
be \textit{registered} to the runtime system by providing information
such as a pointer on the data, its size and type. These information
allows StarPU to automatically perform the data transfer between the
memory nodes during the execution. For example, when data needs to be
accessed on a GPU device, the runtime system automatically transfer it
to the device memory node. As a results StarPU is capable of ensuring
data consistency over multiple nodes. When all the tasks have been
submitted to the runtime system, we wait for their completion by
calling the routine \texttt{starpu\_task\_wait\_for\_all}.

\begin{figure}[!h]
  \lstset{language=C, procnamekeys={},escapechar=>}
  \centering \lstinputlisting{listings/stf-starpu-example.c}
  \caption{\label{fig:stf-starpu-example}Simple example of a parallel version
    of the sequential code in Figure~\ref{fig:seq-example} using a STF
    model with \starpu.}
\end{figure}

\section{Parallelisation of a task-based Cholesky factorization using an STF programming model}
\label{sec:experiments}

In this section we present the two parallel implementations of our
task-based Cholesky solver with both OpenMP and the StarPU runtime
system. These implementations are based on the same STF-based code
corresponding to the pseudo-code shown in
Figure~\ref{fig:spllt-facto-pseudocode}. In this code the sparse
Cholesky factorization is decomposed into six different type of task
associated with the following kernels:

\begin{itemize}
\item \texttt{alloc(snode)}: allocates the data structures
  such as the blocks containing the factors.
\item \texttt{init(snode)}: initializes the supernode \texttt{snode}
  such as copying the coefficient from the original matrix into the
  blocks.
\item \texttt{factorize(bc\_kk)}: factorizes the diagonal block \texttt{bc\_kk}. %% This
  %% is done with the \texttt{potrf} routine from LAPACK.
\item \texttt{solve(bc\_kk, bc\_ik)}: performs the triangular solve of
  an off-diagonal block \texttt{bc\_ik} with the block resulting from
  the factorization of the diagonal block \texttt{bc\_kk} in its
  column.
\item \texttt{update(bc\_ik, bc\_jk, bc\_ij)}: performs the update
  operation of a block \texttt{bc\_ij} within a supernode using the
  blocks \texttt{bc\_ik} and \texttt{bc\_jk} from a column previously
  processed.
\item \texttt{update\_between(snode, bc\_ik, bc\_jk, anode, bc\_ij)}:
  performs the update operation of the block \texttt{bc\_ij} from the
  supernode \texttt{anode} with the blocks \texttt{bc\_ik} and
  \texttt{bc\_jk} from the descendant supernode \texttt{snode}.
\end{itemize}

Following the sequential algorithm, the supernodes are processed
according to a topological order (e.g. a post-order) and similarly the
numerical tasks are submitted in the same order as in sequential
mode. In order to guarantee the correctness of the algorithm, the
runtime system ensures the sequential consistency by using the
data-access information that we provided with the task
submission. Note that the \texttt{alloc} task is done sequentially
because we need to allocate the data structure of blocks to be able to
submit tasks working on these blocks. In the case of \openmp, blocks
are identified using data pointers and these pointer are associated
with a data access when generating a task. Therefore we must allocate
blocks before being able to submit tasks manipulating them and this is
done in this \texttt{alloc} routine. In the case of \starpu, blocks
are associated with a handle that is set up in the \texttt{alloc}
routine. Tasks are then associated with this handle instead of using a
pointer as we do with \openmp. They are several advantages associated
with the use of this handle. For example \starpu is capable of
detecting when data are written for the first time and allocate it
using the information contained in the handle.

The \texttt{init} task is responsible for setting up the blocks in a
given supernode. In our code a supernode is represented by a symbolic
data structure denoted \texttt{snode} and we indicate that we modify
this structure by using the \textit{write} data-access in the
\texttt{init} task. This \texttt{snode} symbolic structure represent
the information associated with a supernode as well as the blocks
associated with the nodal matrix.

\begin{figure}[!h]
  \centering \lstinputlisting{listings/spllt_facto_stf.f90}
\caption{\label{fig:spllt-facto-pseudocode}Pseudo-code for the
  sparse Cholesky factorization using a STF model.}
\end{figure}

\section{Experimental results}\label{sec:experiments}

\begin{table}[htbp]

  \begin{center}
    \begin{tabular}{rl|rrrrl}
      \hline
      \# & Name & n        & nz(A)    & nz(L)    & Flops    & Application/Description \\
         &      & ($10^3$) & ($10^6$) & ($10^6$) & ($10^9$) &                         \\
      \hline
    \end{tabular}
\end{center}

  \caption{Test matrices.}
\end{table}


\begin{table}[htbp]
    \begin{center}
      \input{data/cn255/table_cmp_facto.tex}
    \end{center}
    \caption{Factorization times (seconds) obtained with MA87 and
      SpLLT (i.e. MA87\_starpu). The factorizations were run with the
      block sizes \texttt{nb=(256, 384, 512, 768, 1024)} on 28 cores
      and \texttt{nemin=32}. The lowest factorization times are shown
      in bold.}
\end{table}

\section{Concluding remarks}\label{sec:conclusions}
This report has described in detail the development of a new

 
\section*{Code Availability}
A development version of the Cholesky factorization software used in this 
paper may be checked out of
our source code repository using the following command:

\begin{verbatim}
   svn co -r612 http://ccpforge.cse.rl.ac.uk/svn/spral/branches/xxxxxxx
\end{verbatim}

This code has not yet been optimised and so is not yet
part of the HSL or SPRAL libraries that are we develop
and maintain at the Rutherford Appleton Laboratory (see
\url{http://www.hsl.rl.ac.uk/} and \url{http://www.numerical.rl.ac.uk/spral/}).

\clearpage
\bibliography{flipflapflopBib}

\appendix

\section{Test problems}\label{appendix}
\setcounter{equation}{0}
\setcounter{table}{0}
\setcounter{figure}{0}
\setcounter{table}{0}
\renewcommand{\thetable}{A.\arabic{table}}


In Table~\ref{Tbl:Problems} we list  our test problems along with 
their characteristics. The problems are from the 
University of Florida Sparse Matrix Collection  and are chosen 
to represent a wide range of sparsity structures.

\end{document}
