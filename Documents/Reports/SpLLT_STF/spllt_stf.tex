\documentclass{article}
\pagenumbering{arabic}

\usepackage{url}
\usepackage{color}
\usepackage{verbatim}
\usepackage{tikz}
\usepackage{gnuplot-lua-tikz}

\usepackage{xspace}

%\usepackage[utf8]{inputenc}
%\usepackage{fontspec}
\usepackage{pgfplots}

\usepackage[procnames]{listings}
\lstset{ %
  backgroundcolor=\color{gray98},    % choose the background color; you must add \usepackage{color} or \usepackage{xcolor}
  basicstyle=\tt\small, % \prettysmall      % the size of the fonts that are used for the code
  breakatwhitespace=false,          % sets if automatic breaks should only happen at whitespace
  breaklines=true,                  % sets automatic line breaking
  showlines=true,                  % sets automatic line breaking
  captionpos=b,                     % sets the caption-position to bottom
  commentstyle=\color{gray30},      % comment style
  extendedchars=true,               % lets you use non-ASCII characters; for 8-bits encodings only, does not work with UTF-8
  frame=single,                     % adds a frame around the code
  keepspaces=true,                  % keeps spaces in text, useful for keeping indentation of code (possibly needs columns=flexible)
  keywordstyle=\color{amblu},       % keyword style
  procnamestyle=\color{amred},       % procedures style
  language=[95]fortran,             % the language of the code
  numbers=left,                     % where to put the line-numbers; possible values are (none, left, right)
  numbersep=5pt,                    % how far the line-numbers are from the code
  numberstyle=\tiny\color{gray20}, % the style that is used for the line-numbers
  rulecolor=\color{gray20},          % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. comments (green here))
  showspaces=false,                 % show spaces everywhere adding particular underscores; it overrides 'showstringspaces'
  showstringspaces=false,           % underline spaces within strings only
  showtabs=false,                   % show tabs within strings adding particular underscores
  stepnumber=2,                     % the step between two line-numbers. If it's 1, each line will be numbered
  stringstyle=\color{amblu},       % string literal style
  tabsize=2,                        % sets default tabsize to 2 spaces
  % title=\lstname,                    % show the filename of files included with \lstinputlisting; also try caption instead of title
  procnamekeys={call}
}

\usepackage{color}
\definecolor{gray98}{rgb}{0.98,0.98,0.98}
\definecolor{gray20}{rgb}{0.20,0.20,0.20}
\definecolor{gray25}{rgb}{0.25,0.25,0.25}
\definecolor{gray16}{rgb}{0.161,0.161,0.161}
\definecolor{gray60}{rgb}{0.6,0.6,0.6}
\definecolor{gray30}{rgb}{0.3,0.3,0.3}
\definecolor{bgray}{RGB}{248, 248, 248}
\definecolor{amgreen}{RGB}{77, 175, 74}
% \definecolor{amblu}{RGB}{72, 88, 102}
\definecolor{amblu}{RGB}{55, 126, 184}
\definecolor{amred}{RGB}{228,26,28}
\definecolor{amyellow}{RGB}{237,177,32}
\definecolor{ampurple}{RGB}{126,47,142}
\newcommand{\mye}[1]{\textcolor{amyellow}{#1}\xspace}
\newcommand{\mgr}[1]{\textcolor{amgreen}{#1}\xspace}
\newcommand{\mbl}[1]{\textcolor{amblu}{#1}\xspace}
\newcommand{\mre}[1]{\textcolor{amred}{#1}\xspace}
\newcommand{\mbk}[1]{\textcolor{black}{#1}\xspace}
\newcommand{\mbp}[1]{\textcolor{ampurple}{#1}\xspace}

\input{nlafet_style.sty}

\newcommand{\starpu}{{StarPU}\xspace}
\newcommand{\parsec}{{PaRSEC}\xspace}
\newcommand{\TODO}{\alert{TODO}\xspace}
\newcommand{\openmp}{OpenMP\xspace}
\newcommand{\ma}{HSL\_MA87\xspace}
\newcommand{\spllt}{SpLLT\xspace}

\bibliographystyle{siam}

%-----------------------------------------------------------------------
%
% include macros
%
\input nlafet_macros.tex
%-----------------------------------------------------------------------



\newcommand{\stfccovertitle}
{Experiments with sparse Cholesky using runtime systems}


\newcommand{\theabstract}{We describe the development of a prototype code for 
the solution of large symmetric positive definite sparse systems that is
efficient on parallel architectures.
}

\textwidth  16.18cm
\textheight 23.4cm
\oddsidemargin -0.2mm
\evensidemargin -0.2mm
\def\baselinestretch{1.1}
\topmargin -8.4mm

\newcommand{\n}[1][1]{\vspace*{#1\baselineskip}\noindent}
\newcommand{\ISD}[1]{\begin{center}\n[.4]\textcolor{red}{ISD\
 \hspace*{0.15em} \fbox{\parbox{.8\textwidth}{#1}}}\n[.4]\end{center}}
\newcommand{\HSD}[1]{\begin{center}\n[.4]\textcolor{green}{FL\
\hspace*{0.15em} \fbox{\parbox{.9\textwidth}{\texttt{#1}}}}\n[.4]\end{center}}
\newcommand{\ASH}[1]{\begin{center}\n[.4]\textcolor{brown}{JH\
\hspace*{0.15em} \fbox{\parbox{.9\textwidth}{\texttt{#1}}}}\n[.4]\end{center}}

\newcommand{\metis}{{\sc Me$\!$T$\!$iS\ }}

\hyphenpenalty=10000
\widowpenalty=10000
\sloppy

\begin{document}


\begin{titlepage}

\vspace*{-0.5cm}

\vspace{1.0 cm}

{\Large \bf
\begin{center}
   \stfccovertitle
\end{center}}

\begin{center}
\mbox{} \\
      Iain Duff\footnotemark[1], 
      Jonathan Hogg\footnotemark[1], and Florent Lopez\footnotemark[1]
     
\mbox{} \\
\end{center}

\vspace{1.0cm}


\noindent
{\large ABSTRACT}

\vspace{0.3cm}
\noindent
\theabstract

\vspace{0.6cm}

\begin{description}
\item [Keywords:] sparse Cholesky, SPD systems, runtime systems, STARPU, OpenMP
\item [AMS(MOS) subject classifications:]  65F30, 65F50
\end{description}

\vspace{0.1 cm}

\noindent \rule{15cm}{0.001in}
\vspace{0.1 cm}

\begin{description}

\item [$^1$] Scientific Computing Department, STFC Rutherford 
Appleton Laboratory,
Harwell Campus,\\ Oxfordshire, OX11 0QX, UK.
\end{description}
\noindent
Correspondence to: florent.lopez@stfc.ac.uk\\
This work was supported by the FET-HPC H2020 NLAFET grant number xxxx.\\


\vspace{1.1 cm}
\noindent \today

\end{titlepage}

\newpage
\setcounter{page}{1}
\pagenumbering{arabic}

\section{Introduction} \label{sec:introduction}

In this work we investigate the use of a runtime system for the
implementation of a sparse Cholesky solver with the aim of efficiently
exploiting multicore architectures. Since their introduction multicore
processors have become increasingly popular and can be found nowadays
in the vast majority of high performance computing platforms. Despite
their popularity, exploiting the capabilities of multicore processors
remain a challenge. To cope with multicore hardware, DAG-based
algorithms have been shown to be extremely efficient in terms of
performance and scalability. Initially they have been employed in the
context of dense linear algebra such as in the
PLASMA~\cite{a.d.d.h.ea:09} software package. Motivated by the high
efficiency provided by these algorithms, they have been adapted to
sparse algorithms with the example of the \texttt{HSL\_MA87}
solver~\cite{h.r.s:10} that implements a DAG-based sparse Cholesky
factorization and the \texttt{qr\_mumps} solver~\cite{b:13} implements
a DAG-based multifrontal QR method.

Classically, the development of task-based algorithms includes the
development of an adhoc scheduler which relies on the knowledge of the
algorithm and is implemented using a low-level multithreading library
such as pthread (POSIX threads) to manage synchronisations and enforce
dependencies between processes. In this work, instead, we explore an
alternative approach based on the use of a runtime system which can be
seen as a software layer between the architecture and the
application. In this context the application is then implemented using
a high-level API provided by the runtime system and low level details
such as data consistency across the architecture and task scheduling
are delegated to the runtime system. In this work, we focus on the
exploitation of a Sequential Task Flow (STF) programming model which
allows us to simply express the parallel code from the sequential
one. We show that this STF model, implemented with a task-based
runtime system, can lead to the implementation of a complex algorithm
such as a sparse matrix factorization which is as efficient as a
state-of-the-art solver on a multicore system.

\section{Sparse Cholesky factorization}\label{sec:chol}
\setcounter{equation}{0}
\setcounter{table}{0}
\setcounter{figure}{0}

In this section we describe the supernodal Cholesky method that we use
for solving sparse SPD systems. In particular we will focus on a
DAG-based variant of this algorithm that has been proven to be
extremely efficient on multicore architectures such as
in~\cite{h.r.s:10} the \ma solver that will constitutes the basis of
this work.

The supernodal method is a factorization algorithm for sparse matrices
that relies on an \textit{elimination tree} that expresses the
dependencies during the elimination. The elimination tree is computed
during an analysis phase preceding the factorization and in our case
we obtain it by using the software package HSL\_MC78. At each node of
the tree there is a dense matrix referred to as \textit{nodal} matrix
or \textit{supernode} associated with a set of unknowns. A supernode
corresponds to a set of columns of the matrix that have a similar
nonzero pattern. The factorization is effected by traversing the
elimination tree in a topological order and performing a dense
Cholesky factorization on supernodes and then update the ancestor
nodes with respect to the nodal factorization. Note that they are
several strategies for performing updates such as
\textit{right-looking} updates where ancestors nodes are updated as
soon as the nodal factorization is done, or, \textit{left-looking}
updates where the current supernode is updated just before being
factorized.

\section{Sequential Task Flow parallel programming model}\label{sec:runtime}

In this work we propose exploiting a \textit{Sequential Task Flow
  (STF)} programming model for the implementation of a parallel
task-based Cholesky factorization on top of a runtime system. In this
model the detection of dependencies between tasks relies on a data
analysis of input and output data in order to guarantee the
\textit{sequential consistency} of operations during a parallel
execution. This analysis is often referred to as a
\textit{superscalar} analysis in reference to the dependency detection
between instructions that are performed in superscalar processors. In
this context the dependency graph is used to allow the parallel
execution of independent instructions which is referred to as
instruction-level parallelism and increases the instruction
throughput. The STF model is the most commonly used paradigm for the
parallelization of DAG-based algorithms that have become more and more
popular in the scientific computing community. For example, several
dense linear algebra software packages such as
PLASMA~\cite{a.d.d.h.ea:09} and FLAME~\cite{i.c.q.q.ea:12} use this
paradigm in their implementation. One reason for such a popularity is
that it is extremely easy to use this model as the parallel code is
very similar to the sequential one. Essentially, for a given
sequential algorithm, the STF code is implemented by replacing the
function calls (i.e. the execution of tasks in the case of a DAG-based
algorithm) with the asynchronous submission of this task to a runtime
system in charge of the scheduling. Depending on the data access
provided, the runtime system automatically detects the dependencies
between the tasks. The sequential consistency is then ensured by the
fact that the order of submission of tasks corresponds to the
sequential order.

\begin{figure}[!h]
  \begin{minipage}{0.5\textwidth}
    \centering \lstset{language=C, procnamekeys={}}
    \lstinputlisting{listings/seq-example.c}
    \caption{\label{fig:seq-example}Simple of a sequential code}
    \vspace{0.5cm}
    \centering \lstinputlisting{listings/stf-example.c}
    \caption{\label{fig:stf-example}STF code}
  \end{minipage}
  \hspace{0.5cm}
  \begin{minipage}{0.5\textwidth}
    \centering
    \includegraphics[width=0.7\textwidth]{figures/example_dag}
    \caption{\label{fig:dag-example}DAG corresponding to the
      sequential code presented in Figure~\ref{fig:stf-example}}
  \end{minipage}
  %% \caption{\label{fig:seq-example} Simple example of a sequential code
  %%   on the left with the corresponding DAG represented on the right.}
\end{figure}

As an example, we consider the sequential code in
Figure~\ref{fig:seq-example} for which the corresponding DAG is
represented in Figure~\ref{fig:dag-example}. Based on a STF model, the
parallel version of this code is illustrated in
Figure~\ref{fig:stf-example}. In the sequential code, the two
functions \textit{f} and \textit{g} manipulate arrays \textit{x} and
\textit{y}. The STF code is obtain by submitting the tasks that
consist of a kernels funtion (\textit{f} or \textit{g} in this
example) together with data which are associated with a data access
which can be \textit{R} when the data is read, \textit{W} when the
data is modified, and \textit{RW} when the data is read and modified.

As we have seen with this simple example, the STF model is extremely
easy to use and this characteristic is certainly one of the main
reasons for its popularity in the HPC community. This model, however,
has several drawbacks that may affect the performance and scalability
of parallel codes relying on it. The task are issued and submitted to
the runtime system sequentially. In the case where, for a given DAG,
the task granularity is small compared to the time needed for building
and submitting a task, the parallel execution might be constrained
by the time spent in the submission loop that is setting up the
DAG. To avoid this issue, it might be interesting to consider a
\textit{recursive} model where intermediate tasks might be used to
submit other tasks enabling the distribution of the cost for building
the DAG between the resources instead of doing it in a single
submission loop. This might be implemented for example using
\textit{callback} functions that are executed upon task completion and
can trigger the submission of tasks depending on the task that just
finished its execution. Another issue arising with the STF model comes
from the fact that the whole DAG is unrolled during the parallel
execution and every task in the DAG is stored in order to track task
dependencies. In the case where the DAG is extremely large, handling
and storing the DAG might represent an important overhead in terms of
computational cost and memory storage. Even if the recursive model
allows us to mitigate the problem it doesn't remove it, and it may be
necessary to consider a radically different model such as the
Parametrized Task Graph (PTG) model introduced in~\cite{c.l:95}. In
this model, task dependencies are explicitly encoded with the dataflow
of each task and as a results the whole DAG can be expressed in a
compact format.

%% In the present paper we mainly focus on the 

\section{Runtime systems}\label{sec:runtime}
\setcounter{equation}{0}
\setcounter{table}{0}
\setcounter{figure}{0}

The popularity of task-based algorithms persuaded the OpenMP board to
introduce the \textit{task} construct in Version 3.0 its API. Then
motivated by the popularity of the STF model, the OpenMP board decided
to include the \textit{depend} construct in Version 4.0 allowing users
to express dependencies between tasks in a similar way to the STF
model. In this work we propose an OpenMP implementation of our
Cholesky solver and show advantages of using this in terms of
performance, scalability and productivity. However, because many
features are still unavailable in the OpenMP standard we also
developed another version based on the StarPU runtime system. Both
implementations of our solver rely on a STF model, but the
StarPU-based implementation can benefit from a wider range of features
available with StarPU. For example, although we focus on shared-memory
architectures in this work, the StarPU version might be extended to a
distributed-memory version whereas OpenMP doesn't provide any
possibility for execution on distributed memory architectures.

We present an example of a parallel implementation for the sequential
code in Figure~\ref{fig:seq-example} using OpenMP in
Figure~\ref{fig:stf-openmp-example}.

\begin{figure}[!h]
  \lstset{language=C, procnamekeys={},escapechar=>}
  \centering \lstinputlisting{listings/stf-openmp-example.c}
  \caption{\label{fig:stf-openmp-example}Simple example of a parallel version
    of the sequential code in Figure~\ref{fig:seq-example} using a STF
    model with \openmp.}
\end{figure}

In order to introduce the features provided by the StarPU API, we show
in Figure~\ref{fig:stf-starpu-example} an example of a StarPU-based
implementation for the simple example presented in
Figure~\ref{fig:seq-example}. The task submission is done through the
\texttt{starpu\_insert\_task} function that takes as input a
\textit{codelet} and a set of \textit{handles}. A codelet corresponds
to the description of a task and includes a list of computational
resources where the task can be executed as well as the corresponding
computational kernels. In our example the codelet \texttt{g\_cl} in
line~\ref{code:stf-starpu-example1} describes a task that can be
executed on a CPU and a CUDA device (\texttt{STARPU\_CPU |
  STARPU\_CUDA}) respectively with the kernels \texttt{g\_cpu\_func}
and \texttt{g\_cuda\_func}. The data handles, declared in
line~\ref{code:stf-starpu-example3} in our example, represent a piece
of data that is accessed in the task and can be read
(\texttt{STARPU\_R}), written (\texttt{STARPU\_W}), or read and
written (\texttt{STARPU\_RW}). In order to be used, a data handle must
be \textit{registered} to the runtime system by providing information
such as a pointer on the data, its size and type. These information
allows StarPU to automatically perform the data transfer between the
memory nodes during the execution. For example, when data needs to be
accessed on a GPU device, the runtime system automatically transfer it
to the device memory node. As a results StarPU is capable of ensuring
data consistency over multiple nodes. When all the tasks have been
submitted to the runtime system, we wait for their completion by
calling the routine \texttt{starpu\_task\_wait\_for\_all}.

\begin{figure}[!h]
  \lstset{language=C, procnamekeys={},escapechar=>}
  \centering \lstinputlisting{listings/stf-starpu-example.c}
  \caption{\label{fig:stf-starpu-example}Simple example of a parallel version
    of the sequential code in Figure~\ref{fig:seq-example} using a STF
    model with \starpu.}
\end{figure}

\section{Parallelisation of a task-based Cholesky factorization using an STF programming model}
\label{sec:experiments}

In this section we present the two parallel implementations of our
task-based Cholesky solver with both OpenMP and the StarPU runtime
system. These implementations are based on the same STF-based code
corresponding to the pseudo-code shown in
Figure~\ref{fig:spllt-facto-pseudocode}. In this code the sparse
Cholesky factorization is decomposed into six different type of task
associated with the following kernels:

\begin{itemize}
\item \texttt{alloc(snode)}: allocates the data structures
  such as the blocks containing the factors.
\item \texttt{init(snode)}: initializes the supernode \texttt{snode}
  such as copying the coefficient from the original matrix into the
  blocks.
\item \texttt{factorize(bc\_kk)}: factorizes the diagonal block \texttt{bc\_kk}. %% This
  %% is done with the \texttt{potrf} routine from LAPACK.
\item \texttt{solve(bc\_kk, bc\_ik)}: performs the triangular solve of
  an off-diagonal block \texttt{bc\_ik} with the block resulting from
  the factorization of the diagonal block \texttt{bc\_kk} in its
  column.
\item \texttt{update(bc\_ik, bc\_jk, bc\_ij)}: performs the update
  operation of a block \texttt{bc\_ij} within a supernode using the
  blocks \texttt{bc\_ik} and \texttt{bc\_jk} from a column previously
  processed.
\item \texttt{update\_between(snode, bc\_ik, bc\_jk, anode, bc\_ij)}:
  performs the update operation of the block \texttt{bc\_ij} from the
  supernode \texttt{anode} with the blocks \texttt{bc\_ik} and
  \texttt{bc\_jk} from the descendant supernode \texttt{snode}.
\end{itemize}

Following the sequential algorithm, the supernodes are processed
according to a topological order (e.g. a post-order) and similarly the
numerical tasks are submitted in the same order as in sequential
mode. In order to guarantee the correctness of the algorithm, the
runtime system ensures the sequential consistency by using the
data-access information that we provided with the task
submission. Note that the \texttt{alloc} task is done sequentially
because we need to allocate the data structure of blocks to be able to
submit tasks working on these blocks. In the case of \openmp, blocks
are identified using data pointers and these pointer are associated
with a data access when generating a task. Therefore we must allocate
blocks before being able to submit tasks manipulating them and this is
done in this \texttt{alloc} routine. In the case of \starpu, blocks
are associated with a handle that is set up in the \texttt{alloc}
routine. Tasks are then associated with this handle instead of using a
pointer as we do with \openmp. They are several advantages associated
with the use of this handle. For example \starpu is capable of
detecting when data are written for the first time and allocate it
using the information contained in the handle.

The \texttt{init} task is responsible for setting up the blocks in a
given supernode. In our code a supernode is represented by a symbolic
data structure denoted \texttt{snode} and we indicate that we modify
this structure by using the \textit{write} data-access in the
\texttt{init} task. This \texttt{snode} symbolic structure represent
the information associated with a supernode as well as the blocks
associated with the nodal matrix.

\begin{figure}[!h]
  \centering \lstinputlisting{listings/spllt_facto_stf.f90}
\caption{\label{fig:spllt-facto-pseudocode}Pseudo-code for the
  sparse Cholesky factorization using a STF model.}
\end{figure}

\section{Experimental results}\label{sec:experiments}

\begin{table}[htbp]

  \begin{center}
    \begin{tabular}{rr}
      \hline
      \# & Name & Flops & Application/Description \\
      \hline
    \end{tabular}
\end{center}

  \caption{Test matrices.}
\end{table}


\begin{table}[htbp]
    \begin{center}
      \input{data/cn255/table_cmp_facto.tex}
    \end{center}
    \caption{Factorization times (seconds) obtained with MA87 and
      SpLLT (i.e. MA87\_starpu). The factorizations were run with the
      block sizes \texttt{nb=(256, 384, 512, 768, 1024)} on 28 cores
      and \texttt{nemin=32}. The lowest factorization times are shown
      in bold.}
\end{table}

\section{Concluding remarks}\label{sec:conclusions}
This report has described in detail the development of a new

 
\section*{Code Availability}
A development version of the Cholesky factorization software used in this 
paper may be checked out of
our source code repository using the following command:

\begin{verbatim}
   svn co -r612 http://ccpforge.cse.rl.ac.uk/svn/spral/branches/xxxxxxx
\end{verbatim}

This code has not yet been optimised and so is not yet
part of the HSL or SPRAL libraries that are we develop
and maintain at the Rutherford Appleton Laboratory (see
\url{http://www.hsl.rl.ac.uk/} and \url{http://www.numerical.rl.ac.uk/spral/}).

\clearpage
\bibliography{flipflapflopBib}

\appendix

\section{Test problems}\label{appendix}
\setcounter{equation}{0}
\setcounter{table}{0}
\setcounter{figure}{0}
\setcounter{table}{0}
\renewcommand{\thetable}{A.\arabic{table}}


In Table~\ref{Tbl:Problems} we list  our test problems along with 
their characteristics. The problems are from the 
University of Florida Sparse Matrix Collection  and are chosen 
to represent a wide range of sparsity structures.

\end{document}
