\documentclass{article}
\pagenumbering{arabic}

\usepackage{url}
\usepackage{color}
\usepackage{verbatim}
\usepackage{tikz}
\usepackage{gnuplot-lua-tikz}

\usepackage{xspace}

%\usepackage[utf8]{inputenc}
%\usepackage{fontspec}
\usepackage{pgfplots}


\usepackage[procnames]{listings}
\lstset{ %
  backgroundcolor=\color{gray98},    % choose the background color; you must add \usepackage{color} or \usepackage{xcolor}
  basicstyle=\tt\small, % \prettysmall      % the size of the fonts that are used for the code
  breakatwhitespace=false,          % sets if automatic breaks should only happen at whitespace
  breaklines=true,                  % sets automatic line breaking
  showlines=true,                  % sets automatic line breaking
  captionpos=b,                     % sets the caption-position to bottom
  commentstyle=\color{gray30},      % comment style
  extendedchars=true,               % lets you use non-ASCII characters; for 8-bits encodings only, does not work with UTF-8
  frame=single,                     % adds a frame around the code
  keepspaces=true,                  % keeps spaces in text, useful for keeping indentation of code (possibly needs columns=flexible)
  keywordstyle=\color{amblu},       % keyword style
  procnamestyle=\color{amred},       % procedures style
  language=[95]fortran,             % the language of the code
  numbers=left,                     % where to put the line-numbers; possible values are (none, left, right)
  numbersep=5pt,                    % how far the line-numbers are from the code
  numberstyle=\tiny\color{gray20}, % the style that is used for the line-numbers
  rulecolor=\color{gray20},          % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. comments (green here))
  showspaces=false,                 % show spaces everywhere adding particular underscores; it overrides 'showstringspaces'
  showstringspaces=false,           % underline spaces within strings only
  showtabs=false,                   % show tabs within strings adding particular underscores
  stepnumber=2,                     % the step between two line-numbers. If it's 1, each line will be numbered
  stringstyle=\color{amblu},       % string literal style
  tabsize=2,                        % sets default tabsize to 2 spaces
  % title=\lstname,                    % show the filename of files included with \lstinputlisting; also try caption instead of title
  procnamekeys={call}
}

\usepackage{color}
\definecolor{gray98}{rgb}{0.98,0.98,0.98}
\definecolor{gray20}{rgb}{0.20,0.20,0.20}
\definecolor{gray25}{rgb}{0.25,0.25,0.25}
\definecolor{gray16}{rgb}{0.161,0.161,0.161}
\definecolor{gray60}{rgb}{0.6,0.6,0.6}
\definecolor{gray30}{rgb}{0.3,0.3,0.3}
\definecolor{bgray}{RGB}{248, 248, 248}
\definecolor{amgreen}{RGB}{77, 175, 74}
% \definecolor{amblu}{RGB}{72, 88, 102}
\definecolor{amblu}{RGB}{55, 126, 184}
\definecolor{amred}{RGB}{228,26,28}
\definecolor{amyellow}{RGB}{237,177,32}
\definecolor{ampurple}{RGB}{126,47,142}
\newcommand{\mye}[1]{\textcolor{amyellow}{#1}\xspace}
\newcommand{\mgr}[1]{\textcolor{amgreen}{#1}\xspace}
\newcommand{\mbl}[1]{\textcolor{amblu}{#1}\xspace}
\newcommand{\mre}[1]{\textcolor{amred}{#1}\xspace}
\newcommand{\mbk}[1]{\textcolor{black}{#1}\xspace}
\newcommand{\mbp}[1]{\textcolor{ampurple}{#1}\xspace}

\input{nlafet_style.sty}

\newcommand{\alert}[1]{\textcolor{red}{#1}\xspace}
\newcommand{\starpu}{{StarPU}\xspace}
\newcommand{\parsec}{{PaRSEC}\xspace}
\newcommand{\TODO}[1]{\alert{TODO: #1}\xspace}
\newcommand{\openmp}{OpenMP\xspace}
\newcommand{\ma}{HSL\_MA87\xspace}
\newcommand{\spllt}{SpLLT\xspace}
\newcommand{\qrm}{\texttt{qr\_mumps}\xspace}
\newcommand{\pastix}{\texttt{PaSTIX}\xspace}
\newcommand{\nb}{\texttt{nb}\xspace}

\bibliographystyle{siam}

%-----------------------------------------------------------------------
%
% include macros
%
\input nlafet_macros.tex
%-----------------------------------------------------------------------



\newcommand{\stfccovertitle}
{Experiments with sparse Cholesky using runtime systems}


\newcommand{\theabstract}{We describe the development of a prototype
  code for the solution of large symmetric positive definite sparse
  systems that is efficient on parallel architectures. We implement a
  DAG-based Cholesky factorization which offer good performance and
  scalability on multicore architecture.  Our approach consists in
  using a runtime system for expressing the DAG corresponding to the
  factorization algorithm. The runtime system, play the role of a
  software layer between the application and the architecture and
  handle the management of task dependencies as well as the task
  scheduling. In this model, the application is expressed with a
  high-level API, independent for the hardware details thus enabling
  portability across different architectures. Although largely
  employed in the context of dense linear algebra, this approach is
  still challenging is the context of sparse algorithm because of the
  irregularity of the DAGs arising in these problems. We address this
  challenge by exploiting a Sequential Task Flow model and we
  implement a DAG-based supernodal Cholesky factorization in two
  different version, one implemented with the \openmp standard, and,
  one implemented with the modern runtime system \starpu. We compare
  these two implementation to the state-of-the-art \ma solver and show
  that our implementation offer comparable performance results on a
  multicore architecture. }

\textwidth  16.18cm
\textheight 23.4cm
\oddsidemargin -0.2mm
\evensidemargin -0.2mm
\def\baselinestretch{1.1}
\topmargin -8.4mm

\newcommand{\n}[1][1]{\vspace*{#1\baselineskip}\noindent}
\newcommand{\ISD}[1]{\begin{center}\n[.4]\textcolor{red}{ISD\
 \hspace*{0.15em} \fbox{\parbox{.8\textwidth}{#1}}}\n[.4]\end{center}}
\newcommand{\HSD}[1]{\begin{center}\n[.4]\textcolor{green}{FL\
\hspace*{0.15em} \fbox{\parbox{.9\textwidth}{\texttt{#1}}}}\n[.4]\end{center}}
\newcommand{\ASH}[1]{\begin{center}\n[.4]\textcolor{brown}{JH\
\hspace*{0.15em} \fbox{\parbox{.9\textwidth}{\texttt{#1}}}}\n[.4]\end{center}}

\newcommand{\metis}{{\sc Me$\!$T$\!$iS\ }}

\hyphenpenalty=10000
\widowpenalty=10000
\sloppy

\begin{document}


\begin{titlepage}

\vspace*{-0.5cm}

\vspace{1.0 cm}

{\Large \bf
\begin{center}
   \stfccovertitle
\end{center}}

\begin{center}
\mbox{} \\
      Iain Duff, 
      Jonathan Hogg, and Florent Lopez
     
\mbox{} \\
\end{center}

\vspace{1.0cm}


\noindent
{\large ABSTRACT}

\vspace{0.3cm}
\noindent
\theabstract

\vspace{0.6cm}

\begin{description}
\item [Keywords:] sparse Cholesky, SPD systems, runtime systems, STARPU, OpenMP
\item [AMS(MOS) subject classifications:]  65F30, 65F50
\end{description}

\vspace{0.1 cm}

\noindent \rule{15cm}{0.001in}
\vspace{0.1 cm}

\begin{description}

\item Scientific Computing Department, STFC Rutherford Appleton
  Laboratory, Harwell Campus,\\ Oxfordshire, OX11 0QX, UK.
\end{description}
\noindent
Correspondence to: florent.lopez@stfc.ac.uk\\
This work was supported by the FET-HPC H2020 NLAFET grant number xxxx.\\


\vspace{1.1 cm}
\noindent \today

\end{titlepage}

\newpage
\setcounter{page}{1}
\pagenumbering{arabic}

\section{Introduction} \label{sec:introduction} 



We investigate the use of a runtime system for the implementation of a
sparse Cholesky solver used to find the solution of the linear system:
\begin{equation}\label{eq:linear-system}
  Ax = b
\end{equation}
where $A$ is a large sparse symmetric positive-definite matrix. We
particularly focus on exploiting multicore architectures that have
become increasingly popular since their introduction and can be found
nowadays in the vast majority of high performance computing
platforms. Despite their popularity, exploiting the capabilities of
multicore processors remains a challenge and to cope with multicore
hardware, DAG-based algorithms have been shown to be extremely
efficient in terms of performance and scalability. Initially they have
been employed in the context of dense linear algebra such as in the
PLASMA software package~\cite{a.d.d.h.ea:09}, motivated by the high
efficiency provided by these algorithms. They have been adapted to
sparse algorithms in for example of the \texttt{HSL\_MA87}
solver~\cite{h.r.s:10} that implements a DAG-based sparse supernodal
Cholesky factorization and the \texttt{qr\_mumps} solver~\cite{b:13}
that implements a DAG-based multifrontal QR method.

The traditional approach for implementing a task-based solver includes
the development of an ad hoc scheduler which relies on the knowledge
of the algorithm and is implemented using a low-level multithreading
library such as pthread (POSIX threads) to manage synchronisations and
enforce dependencies between processes. This approach however causes a
lack of portability of the implemented software as it is developed for
a given target architecture, it can then be costly in terms of
programming effort to port the code to different architectures and
adapt to the emerging ones. This problem occurs, for example, when
targeting heterogeneous architectures such as GPU-accelerated
multicore systems that have a more complex memory hierarchy than
traditional multicore systems and equipped different types of
processing units with different capabilities. Instead, in this work,
we explore an alternative approach based on the use of a runtime
system which consists of a software layer between the architecture and
the application. In this context the application is implemented using
a high-level API provided by the runtime system and low level details
such as data consistency across the architecture and task scheduling
are delegated to the runtime system. Several dense linear algebra
software packages were build with using this approach such as
DPLASMA~\cite{b.b.d.f.ea:11}, which is developed with the
\parsec~\cite{b.b.d.f.ea:13} runtime system and Chameleon which
supports several runtime systems including \starpu~\cite{a.t.n.w:11}
and \parsec. Both packages are deigned for distributed memory system
equipped with accelerators. In the context of sparse linear algebra
however, relatively few libraries adopted a this approach. Two
examples of runtime-based sparse solvers are \qrm~\cite{a.b.g.l.:14},
which implements a multifrontal QR method, and \pastix~\cite{h.r.r:02}
which implements a supernodal Cholesky method. Compared to the context
of dense linear algebra, the difficulty for employing this approach in
the sparse case steams from the fact that DAGs are extremely irregular
with a large variability of task granularity and irregularities in the
dependency pattern.

We focus on a Sequential Task Flow (STF) programming model for
expressing the DAG corresponding to our application. This model, also
used in \qrm, offers a simple way to express the parallel code from
the sequential one. Although this work is closely related to the
approach employed in the \starpu version of \pastix, the expression of
dependencies differs in these two solvers. In the \pastix solver,
dependencies are explicitly expressed and therefore doesn't take
advantage of the STF features available in \starpu. We show that our
codes, implemented with a task-based runtime system supporting the STF
model, can lead to an implementation of a sparse matrix factorization
which is as efficient as a state-of-the-art solver on a multicore
system.

%% advantage of GPU-accelerated arch? computation rate, energy
%% efficiency, etc

% computational intensity and device utilisation of kernels on the
% GPU

\section{Task-based sparse Cholesky factorization}\label{sec:chol}
\setcounter{equation}{0}
\setcounter{table}{0}
\setcounter{figure}{0}

We first describe the supernodal Cholesky factorization method that we
use for solving sparse symmetric positive-definite systems. In
particular we will focus on a DAG-based variant of this algorithm that
has been proven to be extremely efficient on multicore architectures
such as for example the \ma solver~\cite{h.r.s:10}. The factorization
is one of the three main phases for solving a linear system which
includes an analysis phase, preceding the factorization, and a solve
phase following it. We specifically focus in the factorization because
it correspond to the most computationally expensive one.

The supernodal Cholesky method is a factorization algorithm for sparse
matrices where the input matrix $A$ is decomposed as:
\begin{equation}\label{eq:chol}
  A = LL^{T}
\end{equation}
where the factor $L$ is a lower triangular matrix. The factorization
is then followed by a solve phase for computing $x$ using the computed
factor by means of forward an backward substitutions. These two steps
consists in solving first, the system $Ly=x$ and second, the system
$L^{T}x=y$. Note that the sparsity pattern of the factor is not the
same as the original matrix as because additional coefficients
generally appear during the computation of this factor resulting in a
denser matrix than the original one. This phenomenon is called
\textit{fill-in} and can be greatly reduced by the use of a symmetric
permutation applied to the matrix before the factorization. If, for
example, we apply the symmetric permutation $P$ to the matrix $A$ then
we transform the original matrix into $PAP^{T}$. The two main
techniques for determining the fill-reducing symmetric permutation are
based on either Minimum Degree~\cite{t.w:67, l:85, a.d.d:96, a.d.d:04}
or Nested Dissection~\cite{g.l:78} methods. The dependencies between
the coefficients in the factor $L$ during the factorization can be
expressed in a tree structure called an \textit{elimination tree}
where each node in this tree represent a column in the factor. In
order to increase the efficiency of operations by exploiting Level 3
BLAS routines, the elimination tree is transformed into an
\textit{assembly tree} where columns having a similar nonzero pattern
are amalgamated into a dense matrix that is referred to a
\textit{nodal} matrix or \textit{supernode}. 

The factorization is effected by traversing the assembly tree in a
topological order and performing two main operations at each
supernode: a dense Cholesky factorization of the current supernode and
an update of the ancestor supernodes using the computed factors in the
nodal factorization. The update operations may be performed according
to several strategies: \textit{right-looking} updates where ancestors
nodes are updated as soon as the nodal factorization is done, or
\textit{left-looking} updates where the current supernode is updated
just before being factorized. The assembly tree is computed during the
analysis phase preceding the factorization and in our case we obtain
it by using the software package HSL\_MC78~\cite{h.s:10}.

There are two main sources of parallelism that can be exploited in the
assembly tree which are the \textit{tree-level} and
\textit{node-level} parallelism. The tree-level parallelism is due to
the fact that supernodes located in separate branches of the tree can
be processed independently and the node-level parallelism is exploited
when nodes are large enough to be efficiently processed by multiple
resources in parallel. One approach for the parallelization of the
supernodal algorithm consists in exploiting these two levels of
parallelism independently. For example, several processes can be used
to handle the factorization of independent supernodes in the tree and
theses processes can exploit node-level parallelism by using
multithreaded routines. Note that with this strategy, there is a
synchronisation point between the processing of a node and the
processing of its children therefore potentially limiting
parallelism. Instead, in our work, we follows the approach proposed
in~\cite{h.r.s:10} where supernodes are partitioned into square blocks
of order \texttt{nb} and operations are applied on these
blocks. Figure~\ref{fig:etree-simple-part} shows a simple assembly
tree that consists of three supernodes where the dashed lines
represent the block partitioning of
supernodes. Figures~\ref{fig:dag-simple} represent the DAG associated
with the factorization of the tree presented in
Figure~\ref{fig:etree-simple-part}.

\begin{figure}[!h]
  \begin{minipage}{0.5\textwidth}
    \centering
    \includegraphics[width=0.7\textwidth]{figures/etree_simple_part}
    \caption{\label{fig:etree-simple-part}Simple assembly tree with
      three supernodes partitioned into square blocks of order
      \texttt{nb}.}
  \end{minipage}
  \hspace{0.5cm}
  \begin{minipage}{0.5\textwidth}
    \centering
    \includegraphics[width=0.7\textwidth]{figures/dag}
    \caption{\label{fig:dag-simple} DAG corresponding to the
      factorization of the tree in Figure~\ref{fig:etree-simple-part}.}
  \end{minipage}
\end{figure}

As shown in the DAG represented in Figure~\ref{fig:dag-simple} they
are several kernels involved in the factorization of the supernodes:
the tasks denoted \texttt{f} correspond to the computation of the
Cholesky factor of a diagonal block. Tasks denoted \texttt{s} perform
a triangular solve of a subdiagonal block using a factor computed with
a task \texttt{f}. Tasks denoted \texttt{u} perform an update of a
block within a supernode corresponding to the previous factorization
of blocks. Finally, tasks denoted \texttt{a} represent the update
between supernodes with respect to the factorization blocks computed
in a given node.
  % For more details on the kernels, we refer
% to~\cite{h.r.s:10}.

In our code, the DAG, such as the one presented in
Figure~\ref{fig:dag-simple}, replaces the elimination tree for
expressing the dependencies during the computation of factors. Note
that when exploiting separately the node and tree parallelism, it is
not possible to start factorizing a supernode before all of its
descendant have been processed. However, when using the DAG, it is
possible that some tasks in a given node become ready for execution
and are then scheduled while its descendent are still being
processed. Using this DAG-based parallelism it is therefore possible
to pipeline the processing of a given node with the processing of it
ancestors. This additional level of parallelism allowed by the use of
a DAG-based algorithm is referred to as inter-node parallelism.

\begin{figure}[!h]
  \centering \lstinputlisting{listings/spllt_facto_seq.f90}
\caption{\label{fig:spllt-facto-seq-pseudocode}Pseudo-code for the
  sequential version of the task-based sparse Cholesky factorization.}
\end{figure}

The pseudo code corresponding to the task-based Cholesky factorization
is presented in Figure~\ref{fig:spllt-facto-seq-pseudocode}. Note that
this is the sequential algorithm that is used as a basis for the
implementation of parallel code. In this code we have the following
kernels:

\begin{itemize}
\item \texttt{alloc(snode)}: partition the supernode \texttt{snode}
  into blocks and allocates the data structures.
\item \texttt{init(snode)}: initializes the blocks by copying the
  coefficients from the original matrix into them.
\item \texttt{factorize(bc\_kk)}: compute the Cholesky factor of the
  diagonal block \texttt{bc\_kk}.
%% This is done with the \texttt{potrf} routine from LAPACK.
\item \texttt{solve(bc\_kk, bc\_ik)}: performs the triangular solve of
  an off-diagonal block \texttt{bc\_ik} with the block resulting from
  the factorization of the diagonal block \texttt{bc\_kk} in its
  column.
\item \texttt{update(bc\_ik, bc\_jk, bc\_ij)}: performs the update
  operation of a block \texttt{bc\_ij} within a supernode using the
  blocks \texttt{bc\_ik} and \texttt{bc\_jk} from a column previously
  processed.
\item \texttt{update\_btw(snode, bc\_ik, bc\_jk, anode, bc\_ij)}:
  performs the update operation of the block \texttt{bc\_ij} from the
  supernode \texttt{anode} with the blocks \texttt{bc\_ik} and
  \texttt{bc\_jk} from the descendant supernode \texttt{snode}.
\end{itemize}

Note that in this algorithm, we perform the update using a
right-looking scheme. Although the left and right looking schemes can
lead to different performance result in serial, none of them is
considered better than the other because their behaviour depend on the
characteristic of the architecture.
 
\section{Sequential Task Flow parallel programming model}\label{sec:stf-model}

In this work we propose exploiting a \textit{Sequential Task Flow
  (STF)} programming model for the implementation of a parallel
task-based Cholesky factorization on top of a runtime system. In this
model the detection of dependencies between tasks relies on a data
analysis of input and output data in order to guarantee the
\textit{sequential consistency} of operations during a parallel
execution. This analysis is often referred to as a
\textit{superscalar} analysis in reference to the dependency detection
between instructions that are performed in superscalar processors. In
this context the dependency graph is used to allow the parallel
execution of independent instructions which is referred to as
instruction-level parallelism and increases the instruction
throughput. The STF model is the most commonly used paradigm for the
parallelization of DAG-based algorithms that have become more and more
popular in the scientific computing community. For example, several
dense linear algebra software packages such as
PLASMA~\cite{a.d.d.h.ea:09} and FLAME~\cite{i.c.q.q.ea:12} use this
paradigm in their implementation. One reason for such a popularity is
that it is extremely easy to use this model as the parallel code is
very similar to the sequential one. Essentially, for a given
sequential algorithm, the STF code is implemented by replacing the
function calls (i.e. the execution of tasks in the case of a DAG-based
algorithm) with the asynchronous submission of this task to a runtime
system in charge of the scheduling. Depending on the data access
provided, the runtime system automatically detects the dependencies
between the tasks. The sequential consistency is then ensured by the
fact that the order of submission of tasks corresponds to the
sequential order.

\begin{figure}[!h]
  \begin{minipage}{0.5\textwidth}
    \centering \lstset{language=C, procnamekeys={}}
    \lstinputlisting{listings/seq-example.c}
    \caption{\label{fig:seq-example}Simple of a sequential code}
    \vspace{0.5cm}
    \centering \lstinputlisting{listings/stf-example.c}
    \caption{\label{fig:stf-example}STF code}
  \end{minipage}
  \hspace{0.5cm}
  \begin{minipage}{0.5\textwidth}
    \centering
    \includegraphics[width=0.7\textwidth]{figures/example_dag}
    \caption{\label{fig:dag-example}DAG corresponding to the
      sequential code presented in Figure~\ref{fig:stf-example}}
  \end{minipage}
  %% \caption{\label{fig:seq-example} Simple example of a sequential code
  %%   on the left with the corresponding DAG represented on the right.}
\end{figure}

As an example, we consider the sequential code in
Figure~\ref{fig:seq-example} for which the corresponding DAG is
represented in Figure~\ref{fig:dag-example}. Based on a STF model, the
parallel version of this code is illustrated in
Figure~\ref{fig:stf-example}. In the sequential code, the two
functions \textit{f} and \textit{g} manipulate arrays \textit{x} and
\textit{y}. The STF code is obtain by submitting the tasks that
consist of a kernels funtion (\textit{f} or \textit{g} in this
example) together with data which are associated with a data access
which can be \textit{R} when the data is read, \textit{W} when the
data is modified, and \textit{RW} when the data is read and modified.

As we have seen with this simple example, the STF model is extremely
easy to use and this characteristic is certainly one of the main
reasons for its popularity in the HPC community. This model, however,
has several drawbacks that may affect the performance and scalability
of parallel codes relying on it. The task are issued and submitted to
the runtime system sequentially. In the case where, for a given DAG,
the task granularity is small compared to the time needed for building
and submitting a task, the parallel execution might be constrained
by the time spent in the submission loop that is setting up the
DAG. To avoid this issue, it might be interesting to consider a
\textit{recursive} model where intermediate tasks might be used to
submit other tasks enabling the distribution of the cost for building
the DAG between the resources instead of doing it in a single
submission loop. This might be implemented for example using
\textit{callback} functions that are executed upon task completion and
can trigger the submission of tasks depending on the task that just
finished its execution. Another issue arising with the STF model comes
from the fact that the whole DAG is unrolled during the parallel
execution and every task in the DAG is stored in order to track task
dependencies. In the case where the DAG is extremely large, handling
and storing the DAG might represent an important overhead in terms of
computational cost and memory storage. Even if the recursive model
allows us to mitigate the problem it doesn't remove it, and it may be
necessary to consider a radically different model such as the
Parametrized Task Graph (PTG) model introduced in~\cite{c.l:95}. In
this model, task dependencies are explicitly encoded with the dataflow
of each task and as a results the whole DAG can be expressed in a
compact format.

%% In the present paper we mainly focus on the 

\section{Runtime systems}\label{sec:runtime}
\setcounter{equation}{0}
\setcounter{table}{0}
\setcounter{figure}{0}

The popularity of task-based algorithms persuaded the OpenMP board to
introduce the \textit{task} construct in Version 3.0 of its API. Then
motivated by the popularity of the STF model, the OpenMP board decided
to include the \textit{depend} construct in Version 4.0 allowing users
to express dependencies between tasks in a similar way to the STF
model presented in Section~\ref{sec:stf-model}. In this work we
propose an OpenMP implementation of our Cholesky solver and show
advantages of using this in terms of performance, scalability and
productivity. However, because many features are still unavailable in
the OpenMP standard, we also developed another version based on the
StarPU runtime system. As shown in the next section, both
implementations of our solver rely on a STF model, but the
StarPU-based implementation can benefit from a wider range of features
available with StarPU. For example, although we focus on shared-memory
architectures in this work, the StarPU version can be extended to a
distributed-memory version whereas OpenMP doesn't provide any
possibility for execution on distributed memory architectures.

\begin{figure}[!h]
  \lstset{language=C, procnamekeys={},escapechar=>}
  \centering \lstinputlisting{listings/stf-openmp-example.c}
  \caption{\label{fig:stf-openmp-example}Simple example of a parallel version
    of the sequential code in Figure~\ref{fig:seq-example} using a STF
    model with \openmp.}
\end{figure}

We present an example of a parallel implementation for the sequential
code in Figure~\ref{fig:seq-example} using OpenMP in
Figure~\ref{fig:stf-openmp-example}. In this example we first create
the parallel section using the omp construct \textit{parallel} and
then we put the master thread in charge of the task submission using
the \textit{master} construct. As previously explained, the task are
created with the \textit{task} construct and the data access are given
to the runtime system using the \textit{depend} construct. In the
\openmp standard read-only data access are indicated with the
parameter \textit{in}, write-only with the parameter \textit{out} and
read-write with the parameter \textit{inout}. Finally the task
submission loop is concluded with the \textit{taskwait} clause
indicating that the master thread should wait for the completion of
the tasks previously submitted.

Similarly to the \openmp example provided in
Figure~\ref{fig:stf-openmp-example}, and in order to introduce the
features provided by the StarPU API, we show in
Figure~\ref{fig:stf-starpu-example} an example of a StarPU-based
implementation for the simple example presented in
Figure~\ref{fig:seq-example}. The task submission is done through the
\texttt{starpu\_insert\_task} function that takes as input a
\textit{codelet} and a set of \textit{handles}. A codelet corresponds
to the description of a task and includes a list of computational
resources where the task can be executed as well as the corresponding
computational kernels. In our example the codelet \texttt{g\_cl} in
line~\ref{code:stf-starpu-example1} describes a task that can be
executed on a CPU and a CUDA device (\texttt{STARPU\_CPU |
  STARPU\_CUDA}) respectively with the kernels \texttt{g\_cpu\_func}
and \texttt{g\_cuda\_func}. The data handles, declared in
line~\ref{code:stf-starpu-example3} in our example, represent a piece
of data that is accessed in the task and can be read
(\texttt{STARPU\_R}), written (\texttt{STARPU\_W}), or read and
written (\texttt{STARPU\_RW}). In order to be used, a data handle must
be \textit{registered} to the runtime system by providing information
such as a pointer on the data, its size and type. These information
allows StarPU to automatically perform the data transfer between the
memory nodes during the execution. For example, when data needs to be
accessed on a GPU device, the runtime system automatically transfer it
to the device memory node. As a results StarPU is capable of ensuring
data consistency over multiple nodes. When all the tasks have been
submitted to the runtime system, we wait for their completion by
calling the routine \texttt{starpu\_task\_wait\_for\_all}.

\begin{figure}[!h]
  \lstset{language=C, procnamekeys={},escapechar=>}
  \centering \lstinputlisting{listings/stf-starpu-example.c}
  \caption{\label{fig:stf-starpu-example}Simple example of a parallel version
    of the sequential code in Figure~\ref{fig:seq-example} using a STF
    model with \starpu.}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/scheduler}
    \caption{\label{fig:scheduler} Illustration of the dynamic
      scheduling strategy of tasks in the runtime system.}
\end{figure}

Both \openmp and \starpu implementations rely on a dynamic scheduler
performing the scheduling of the ready tasks during the execution. In
this model, a task is put in the scheduler as soon as it becomes ready
for execution which is the case when all of its dependencies are
fulfilled. On the other side of the scheduler, workers try to retrieve
a task from the scheduler when they become idle. This dynamic
scheduling strategy is illustrated in Figure~\ref{fig:scheduler} where
the scheduler is placed between the runtime core where the DAG is
build as a result of the task submission and the workers which can be
CPUs and GPUs for example. The scheduler is responsible for storing
the ready tasks in scheduling queues and distributing them to idle
workers. Although \openmp Version 4.0 doesn't provide any features to
control the scheduling policy, the Version 4.5 allows users to provide
a priority along with a submitted task to give hints to the scheduler
so that it can detect the most critical tasks in the DAG. \starpu not
only support the use of task priority but it also give the possibility
to use several scheduling strategies and to implement a new one if
necessary.

\section{Parallelisation of a task-based Cholesky factorization using an STF programming model}
\label{sec:spllt-stf}

%% In this section we describe the STF-based parallel code of our
%% DAG-based Cholesky solver. We developed two different versions using
%% first the OpenMP standard and second the StarPU runtime system whose
%% features are presented in Section~\ref{sec:runtime}.  

%% Using the STF programming model introduced in
%% Section~\ref{sec:stf-model}, we show the implementation of the
%% supernodal Cholesky factorization in
%% Figure~\ref{fig:spllt-facto-pseudocode}.

In this section we describe the implementation of our DAG-based
Cholesky solver using the STF programming parallel model presented in
Section~\ref{sec:stf-model}. We developed two different version of our
code using first, the OpenMP standard and second, the StarPU runtime
system whose features are presented in Section~\ref{sec:runtime}.

A pseudocode of our solver is presented in
Figure~\ref{fig:spllt-facto-pseudocode} and, following the sequential
algorithm shown in Figure~\ref{fig:spllt-facto-seq-pseudocode},
consists in a bottom-up traversal of the assembly tree where at each
node the tasks for the factorization and update operations are
submitted to the runtime system. The kernels used in the tasks, are
the same as the ones presented in section~\ref{sec:chol}. Note that
the task submission is done using a right looking scheme which means
that every nodes in the tree must be allocated and partitioned before
the submission of the numerical tasks. In addition, the \texttt{alloc}
task is executed sequentially because the we need to allocate the data
structures and partition the supernodes in order to submitted the
numerical tasks.

\begin{figure}[!h]
  \centering \lstinputlisting{listings/spllt_facto_stf.f90}
  \caption{\label{fig:spllt-facto-pseudocode}Pseudo-code for the sparse
    Cholesky factorization using a STF model presented in
    Section~\ref{sec:stf-model}.}
\end{figure}

As explained in Section~\ref{sec:stf-model}, when using the STF model
we need to provide the access mode along with the data when submitting
a task so that the runtime can ensures the sequential consistency of
the parallel algorithm. For this reason, in the submission of
\texttt{factorize} tasks, the diagonal block \texttt{blk(k,k)} is
associated with a read-write access mode indicating that the kernel is
going to read and modify this block as it computes the Cholesky factor
of this block. Similarly, as the \texttt{solve} operations require the
diagonal block for computing the factors in the subdiagonal blocks, we
have to indicate that this diagonal block is read when submitting the
\texttt{solve} by associating it with a read-only access mode. With
these information the runtime detects the dependencies between the
\texttt{factorize} and \texttt{solve} tasks and allows the parallel
execution of the solve tasks within a block-column. 

In order to make sure the supernode is initialized before the
factorization starts, we use a symbolic handle called \texttt{snode}
and pass it to the \texttt{init} tasks using a write access mode. Then
we also pass it to the the \texttt{factorize} tasks in a read access
mode and because all the subsequent factorization tasks in a supernode
depend on the first \texttt{factorize} task we guarantee that the
numerical task cannot start before the supernode is initialized. For
the same reason, the \texttt{update\_btw} task takes the
\texttt{anode} handle in input with a read access mode because it
modifies a block in an ancestor node and the task should not be
executed before the node is initialized. The particularity of this
symbolic handle is that it represents a set of blocks instead of a
unique block.

One issue arise with the dependency detection of the \texttt{update}
tasks that are applied on a given block. This task takes in input two
block $L_{ik}$ and $L_{jk}$ and applies on a third block $L_{ij}$ the
following operation:
\begin{equation*}
  L_{ij} = L_{ij} - L_{ik}L_{jk}^{T}
\end{equation*}
and these update operations are clearly commutative. However, when two
\texttt{update} tasks applied to a same block are submitted, the
runtime system detects that these task modify the same data it will
enforce the same order of execution for these tasks than the order of
submission. With StarPU it is possible to avoid these unnecessary
dependency that potentially limit the parallelism using the
\texttt{STARPU\_COMMUTE} flag indicating that operations performed by
a kernel are commutative. The OpenMP standard still does not provide
such a functionality.

Interestingly, the STF code that is presented in
Figure~\ref{fig:spllt-facto-pseudocode} is independent from the
runtime system used for the implementation. In practice only the
implementation of the \texttt{submit} routines are specific to the
runtime system. This illustrate the fact that the expression of the
algorithm is strictly separated from the task scheduling and data
coherency management. An example of the implementation of this
\texttt{submit} routine in the \starpu version is given in
Figure~\ref{fig:spllt-starpu-slv-task}, and its equivalent in the
\openmp version is given in Figure~\ref{fig:spllt-omp-slv-task}. In
this example we show the submission of the solve tasks. In the \openmp
version, blocks are identified using data pointers and these pointer
are associated with a data access when submitting a task. It is thus
necessary to allocate the blocks before being able to submit the tasks
manipulating the blocks. In the case of \starpu, blocks are associated
with a handle that is set up in the \texttt{alloc} routine. Tasks are
then associated with this handle instead of using a pointer as we do
with \openmp. They are several advantages associated with the use of
this handle. For example \starpu is capable of detecting when data are
written for the first time and allocate it using the information
contained in the handle. We don't use this features for the allocation
of block, but we use it for the management of scratch memory that is
needed is the \texttt{update\_btw} task and not represented in the
pseudocode for the sake of clarity.

\begin{figure}[!h]
  \begin{minipage}{0.5\textwidth}
    \centering
    \lstset{basicstyle=\tt\scriptsize, language=C}
    \lstinputlisting{listings/spllt_starpu_solve_task.c}
    \caption{\label{fig:spllt-starpu-slv-task} Submission routine used
      for the solve tasks in the \starpu code}
  \end{minipage}
  \hspace{0.5cm}
  \begin{minipage}{0.5\textwidth}
    \centering
    \lstset{basicstyle=\tt\scriptsize}
    \lstinputlisting{listings/spllt_omp_solve_task.f90}
    \caption{\label{fig:spllt-omp-slv-task}Submission routine used
      for the solve tasks in the \openmp code}
  \end{minipage}
\end{figure}

Note that the efficiency of these submission routines may be critical
in the performance of the execution and as shown in our tests, in some
cases, the submission of tasks in the DAG may be a limiting factor for
the performance. This happens when the number of tasks is important
and the task granularity is small. In such cases, especially when the
number of resources increases, the unrolling of the DAG may be too
slow to feed all the resources therefore bounding the execution
time. In that respect the partition parameter $nb$ may influence the
performance as a small value for this parameter increases the amount
of tasks in the DAG and therefore the overhead associated with the
task submission and also the task management in the runtime system
including the task scheduling.

%% The \texttt{init} task is responsible for setting up the blocks in a
%% given supernode. In our code a supernode is represented by a symbolic
%% data structure denoted \texttt{snode} and we indicate that we modify
%% this structure by using the \textit{write} data-access in the
%% \texttt{init} task. This \texttt{snode} symbolic structure represent
%% the information associated with a supernode as well as the blocks
%% associated with the nodal matrix.

\alert{Should we introduce another section to present the MA87
  scheduler and task dependency management?}

\section{Experimental results}\label{sec:experiments}

In this section we present the experimental results obtained on our
test matrices taken from the University of Florida Matrix
Collection~\cite{d.h:11}. In this collection we selected a large set
of symmetric positive-definite matrices from a wide range of
application and sparsity structure. They are listed in
Table~\ref{table:mat} along with their sizes and number of entries. In
this table we also indicated the number of entries in the factor $L$
and the operation count for the factorization when using the
nested-dissection ordering \metis~\cite{k.k:98}. As explained in
Section~\ref{sec:chol} this allows us to reduce the number of entries
in the factor $L$ by limiting the fill-in. Note that in this table,
matrix characteristics are obtained without node amalgamation. This
means that the number of entries in $L$ as well as the flop count is
minimized. However, in our experiments we use node amalgamation to
obtain a better efficiency of operations at the cost of an increase in
the number of entries in $L$ as well as the operation count. The node
amalgamation is controlled by a parameter \texttt{nemin} used during
the analysis phase and the strategy for amalgamating the nodes in
analysis routine of the MA78 is as follow: the elimination tree is
traversed using a post order and when a node is visited, it is merge
with its parents if the column count in both nodes is lower than
\texttt{nemin} or if the merging generate no additional fill-in in
$L$. In our experiment we set the \texttt{nemin} value to $32$ which
correspond to a good trade-off between sparsity and efficiency of
operations in our experiments.

In this study the tests were made on a multicore machine equipped with
two Intel(R) Xeon(R) E5-2695 v3 CPUs with fourteen cores each (twenty
eight cores in total). Each core, clocked at 2.3 Ghz and is equipped
with AVX2, has a peak of 36.8 GFlop/s corresponding to a total peak of
1.03 TFlop/s in real, double precision arithmetic. The code is
compiled with the GNU compiler (\texttt{gcc} and \texttt{gfortran}),
the BLAS and LAPACK routines are provided by the Intel MKL v11.3
library and we used the trunk development version of the \starpu
runtime system freely available through anonymous SVN access.

The choice for the parameter \nb in the parallel execution is not
trivial as it impacts several aspects of the execution that influences
the performance. Although small value for this parameter increases the
number of tasks in the DAG and thus the parallelism, it also reduces
the performance of the level 3 BLAS routines executed in the
tasks. The optimal value for this parameter thus correspond to a
trade-off between a sufficient amount of parallelism to feed the
resources and a good kernels efficiency. In addition, as we have seen
in Section~\ref{sec:spllt-stf} the parameter \nb influence the
overhead for managing the task because when the number of tasks in the
DAG increases, it also increase the time needed for the handling the
DAG including the task submission, dependency detection and
scheduling. Finally, the optimal value for \nb depend on a large
number of parameters such as processing unit capabilities, number of
resources and cannot be easily determined without a precise
performance model for the application which is, in practice, extremely
difficult to establish. Instead we empirically determine a good \nb
value for each problem by running multiple tests on a range of values
which is \texttt{nb = (256, 384, 512, 768, 1024)} in these experiments.

The best factorization times obtained with \spllt and \ma for every
problem listed in Table~\ref{table:mat} are reported in
Table~\ref{table:exp-times} along with the corresponding value of \nb
for the experiment. The Gflop/s rates corresponding to the best
factorization times are represented in Figures~\ref{fig:exp-perf} for
the different solvers. Additionally Figures~\ref{fig:exp-rel-perf}
illustrate the relative performance of the \spllt codes compared \ma.

For matrices from \#27 to \#38, corresponding to the bigger problems
of our test set, the performance behaviour of the \openmp and \starpu
version of \spllt is very similar and comparable to the one obtained
with our reference solver \ma. On smaller problems, with factorization
times generally smaller than a second, it appears that the \openmp
version gives better results that the \starpu one, with results
competitive with \ma except on some specific problems. Matrices \# 1
and \# 15, for examples, gives extremely poor results with both
version of our code.

%% \begin{figure}[!h]
%%   \begin{minipage}{0.5\textwidth}
%%     \centering
%%     \includegraphics[width=\textwidth]{data/cmp_perf_stf}
%%     \caption{\label{fig:exp-perf}\TODO{put caption}}
%%   \end{minipage}
%%   \hspace{0.5cm}
%%   \begin{minipage}{0.5\textwidth}
%%     \centering
%%     \includegraphics[width=\textwidth]{data/cmp_facto_rel_stf}
%%     \caption{\label{fig:exp-perf}\TODO{put caption}}
%%   \end{minipage}
%% \end{figure}

\begin{table}[htbp]
    \begin{center}
      \texttt{ \input{data/cn255/table_cmp_facto.tex}}
    \end{center}
    \caption{\label{table:exp-times}Factorization times (seconds) obtained with MA87 and
      SpLLT (i.e. MA87\_starpu). The factorizations were run with the
      block sizes \texttt{nb=(256, 384, 512, 768, 1024)} on 28 cores
      and \texttt{nemin=32}. The lowest factorization times are shown
      in bold.}
\end{table}

%\newpage

\begin{figure}[!h]
  \centering
  \includegraphics[width=\textwidth]{data/cmp_perf_stf}
  \caption{\label{fig:exp-perf} Performance results for both \openmp
    and \starpu versions of SpLLT solver and \ma solver on 28 cores
    for our test matrices presented in Table~\ref{table:mat}.}
\end{figure}

\begin{figure}[!h]
  \centering
  \includegraphics[width=\textwidth]{data/cmp_facto_rel_stf}
  \caption{\label{fig:exp-rel-perf}Performance results obtained with
    SpLLT, \openmp and \starpu versions, relatively to the performance
    obtained with \ma.}
\end{figure}

\section{Concluding remarks}\label{sec:conclusions}
This report has described in detail the development of a new

 
%% \section*{Code Availability}
%% A development version of the Cholesky factorization software used in this 
%% paper may be checked out of
%% our source code repository using the following command:

%% \begin{verbatim}
%%    svn co -r612 http://ccpforge.cse.rl.ac.uk/svn/spral/branches/xxxxxxx
%% \end{verbatim}

%% This code has not yet been optimised and so is not yet
%% part of the HSL or SPRAL libraries that are we develop
%% and maintain at the Rutherford Appleton Laboratory (see
%% \url{http://www.hsl.rl.ac.uk/} and \url{http://www.numerical.rl.ac.uk/spral/}).

\clearpage

\bibliography{flipflapflopBib}

\clearpage

\appendix

\section{Test problems}\label{appendix}
\setcounter{equation}{0}
\setcounter{table}{0}
\setcounter{figure}{0}
\setcounter{table}{0}
\renewcommand{\thetable}{A.\arabic{table}}

\begin{table}[htbp]
  \label{table:mat}
  \begin{center}
    \texttt{\begin{tabular}{rl|rrrrl}
      \hline
      \# & Name                            & n        & nz(A)    & nz(L)    & Flops    & Application/Description  \\
         &                                 & ($10^3$) & ($10^6$) & ($10^6$) & ($10^9$) &                          \\
      \hline
      1  & Schmid/thermal2                 & 1228     & 4.9      & 51.6     & 14.6     & Unstructured thermal FEM \\
      2  & Rothberg/gearbox                & 154      & 4.6      & 37.1     & 20.6     & Aircraft flap actuator   \\
      3  & DNVS/m\_t1                      & 97.6     & 4.9      & 34.2     & 21.9     & Tubular joint            \\
      4  & Boeing/pwtk                     & 218      & 5.9      & 48.6     & 22.4     & Pressurised wind tunnel  \\
      5  & Chen/pkustk13                   & 94.9     & 3.4      & 30.4     & 25.9     & Machine element          \\
      6  & GHS\_psdef/crankseg\_1          & 52.8     & 5.3      & 33.4     & 32.3     & Linear static analysis   \\
      7  & Rothberg/cfd2                   & 123      & 1.6      & 38.3     & 32.7     & CFD pressure matrix      \\
      8  & DNVS/thread                     & 29.7     & 2.2      & 24.1     & 34.9     & Threaded connector       \\
      9  & DNVS/shipsec8                   & 115      & 3.4      & 35.9     & 38.1     & Ship section             \\
      10 & DNVS/shipsec1                   & 141      & 4.0      & 39.4     & 38.1     & Ship section             \\
      11 & GHS\_psdef/crankseg\_2          & 63.8     & 7.1      & 43.8     & 46.7     & Linear static analysis   \\
      12 & DNVS/fcondp2                    & 202      & 5.7      & 52.0     & 48.2     & Oil production platform  \\
      13 & Schenk\_AFE/af\_shell3          & 505      & 9.0      & 93.6     & 52.2     & Sheet metal forming      \\
      14 & DNVS/troll                      & 214      & 6.1      & 64.2     & 55.9     & Structural analysis      \\
      15 & AMD/G3\_circuit                 & 1586     & 4.6      & 97.8     & 57.0     & Circuit simulation       \\
      16 & GHS\_psdef/bmwcra\_1            & 149      & 5.4      & 69.8     & 60.8     & Automotive crankshaft    \\
      17 & DNVS/halfb                      & 225      & 6.3      & 65.9     & 70.4     & Half-breadth barge       \\
      18 & Um/2cubes\_sphere               & 102      & 0.9      & 45.0     & 74.9     & Electromagnetics         \\
      19 & GHS\_psdef/ldoor                & 952      & 23.7     & 144.6    & 78.3     & Large door               \\
      20 & DNVS/ship\_003                  & 122      & 4.1      & 60.2     & 81.0     & Ship structure           \\
      21 & DNVS/fullb                      & 199      & 6.0      & 74.5     & 100.2    & Full-breadth barge       \\
      22 & GHS\_psdef/inline\_1            & 504      & 18.7     & 172.9    & 144.4    & Inline skater            \\
      23 & Chen/pkustk14                   & 152      & 7.5      & 106.8    & 146.4    & Tall building            \\
      24 & GHS\_psdef/apache2              & 715      & 2.8      & 134.7    & 174.3    & 3D structural problem    \\
      25 & Koutsovasilis/F1                & 344      & 13.6     & 173.7    & 218.8    & AUDI engine crankshaft   \\
      26 & Oberwolfach/boneS10             & 915      & 28.2     & 278.0    & 281.6    & Bone micro-FEM           \\
      27 & ND/nd12k                        & 36.0     & 7.1      & 116.5    & 505.0    & 3D mesh problem          \\
      28 & JGD\_Trefethen/Trefethen\_20000 & 20.0     & 0.3      & 90.7     & 652.6    & Integer matrix           \\
      29 & ND/nd24k                        & 72.0     & 14.4     & 321.6    & 2054.4   & 3D mesh problem          \\
      30 & Janna/Flan\_1565                & 1565     & 59.5     & 1477.9   & 3859.8   & 3D mechanical problem    \\
      31 & Oberwolfach/bone010             & 987      & 36.3     & 1076.4   & 3876.2   & Bone micro-FEM           \\
      32 & Janna/StocF-1465                & 1465     & 11.2     & 1126.1   & 4386.6   & Underground aquifer      \\
      33 & GHS\_psdef/audikw\_1            & 944      & 39.3     & 1242.3   & 5804.1   & Automotive crankshaft    \\
      34 & Janna/Fault\_639                & 639      & 14.6     & 1144.7   & 8283.9   & Gas reservoir            \\
      35 & Janna/Hook\_1498                & 1498     & 31.2     & 1532.9   & 8891.3   & Steel hook               \\
      36 & Janna/Emilia\_923               & 923      & 21.0     & 1729.9   & 13661.1  & Gas reservoir            \\
      37 & Janna/Geo\_1438                 & 1438     & 32.3     & 2467.4   & 18058.1  & Underground deformation  \\
      38 & Janna/Serena                    & 1391     & 33.0     & 2761.7   & 30048.9  & Gas reservoir            \\
      \hline
    \end{tabular}}
  \end{center}
  \caption{Test matrices and their characteristics without node
    amalgamation. $n$ is the matrix order, $nz(A)$ represent the
    number entries in the matrix $A$, $nz(L)$ represent the number of
    entries the factor $L$ and \textit{Flops} correspond to the operation
    count for the matrix factorization.}
\end{table}

%% In Table~\ref{Tbl:Problems} we list  our test problems along with 
%% their characteristics. The problems are from the 
%% University of Florida Sparse Matrix Collection  and are chosen 
%% to represent a wide range of sparsity structures.

\end{document}
